\documentclass{article}

\usepackage{amsmath,amssymb,graphicx,cite,setspace} % PLOS One TEMPLATE
\usepackage{listings, url, color, appendix} % things I usually use
% items to make todonotes useful
\usepackage[textwidth=1.75in]{todonotes} 
\usepackage[total={6in,9.5in},top=0.5in,left=0.5in,includefoot]{geometry}
\newcommand{\wantFig}[2]{
  \begin{figure}[h]\centering
    \missingfigure{}
    \caption{#2}
    \figlabel{#1}
  \end{figure}
}
% alt layout formatting
%\doublespacing

% Text layout
%\topmargin 0.0cm
%\oddsidemargin 0.5cm
%\evensidemargin 0.5cm
%\textwidth 16cm 
%\textheight 21cm


% define Scala
\lstdefinelanguage{scala}{morekeywords={package,class,object,trait,extends,with,new,if,while,for,def,val,var,this,case,import,type,->,=>},
%otherkeywords={->,=>},
%procnamekeys={def,.},
sensitive=true,
keywordstyle=\color{magenta},
commentstyle=\color{green},
identifierstyle=\color{blue},
%procnamestyle=\color{black},
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morestring=[b]"}
% Default settings for code listings
\lstset{frame=tb,language=scala,aboveskip=3mm,belowskip=3mm,
showstringspaces=false,columns=flexible,basicstyle={\small\ttfamily}}
% handle tilde characters
\lstset{literate=%
{~}{\url{~}}1
}
% define no break code listing
\lstnewenvironment{code}[1][]%
{
   \noindent
   \minipage{\linewidth} 
   \vspace{0.5\baselineskip}
   \lstset{basicstyle=\ttfamily\footnotesize,frame=single,#1}}
{\endminipage}

\newenvironment{rnwfig}[0]{\begin{figure}\begin{center}}{\end{center}\end{figure}}

\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

\newcommand{\Hub}[0]{\ensuremath{\mathbf{H}}}
\newcommand{\C}[1]{\ensuremath{\mathbf{C}_{#1}}}
\newcommand{\Obs}[0]{\ensuremath{\mathbf{O}}}
\newcommand{\todoCP}[1]{\todo{CP, #1}}

\newcommand{\figlabel}[1]{\label{fig_#1}}
\newcommand{\figref}[1]{fig. \ref{fig_#1}}
\newcommand{\Figref}[1]{Fig. \ref{fig_#1}}

\newcommand{\applabel}[1]{\label{app_#1}}
\newcommand{\appref}[1]{appendix \ref{app_#1}}
\newcommand{\Appref}[1]{Appendix \ref{fig_#1}}

\begin{document}
\SweaveOpts{concordance=TRUE}
\begin{flushleft}
{\Large
\textbf{Agent-Based Modeling of Covert Social Groups, Applying Software Engineering Patterns and Planning for Cluster Computation}
}
% Insert Author names, affiliations and corresponding author email.
\\
Carl A. B. Pearson$^{1,\ast}$, 
Edo Airoldi$^{2}$,
Edward Kao$^{2}$,
Burton Singer$^{1}$, 
\\
\bf{1} Emerging Pathogens Institute, University of Florida, Gainesville, FL, USA
\\
\bf{2} Statistics, Harvard University, Cambridge, MA, USA
\\
$\ast$ E-mail: cap10 \{at\} ufl \{dot\} edu
\end{flushleft}
% Please keep the abstract between 250 and 300 words
\section*{Abstract}
We discuss covert social network analysis (SNA) in the larger scientific context, highlight pitfalls, and propose what we view as necessary analysis that seems absent.  Because SNA typically occurs on aggregated observations (e.g., ties are across types and times), and furthermore covert SNA involves at least some (and often severe) censoring of those observations, it is critical that the implications of that aggregation and observation process be considered.

We demonstrate an implementation framework for performing that kind of analysis, based on a distributed, agent-based model.  We also apply past software engineering insights to show how they can make for more natural (and therefore comprehensible and compartmentally testable) agent-based models in this framework.

We close with discussion of how this framework might be used in a practical application, and how it is might be pieced together with other analytical tools.

\newpage

% Covert social groups are difficult to observe directly, by definition: members either actively thwart discovery or leverage passive features of their surrounding population to avoid detection.  They watch the watchers as well, and increasing effort to reveal them should be expected to encourage adaptation.  Yet, as impossible as it is for outsiders to perfectly see the covert group, it is equally impossible for these groups to perfectly hide and accomplish anything of note.


% Their secrecy may make them conspicuously absent from the everyday activity of the background population.  Finally there are potential mechanical charactericterizations -- their composition, goals, tactics, etc. -- that might predict their actions.
% 
% Often these mechanically characterizations are framed in terms of social networks.  Individuals and their ties are thought to have certain graph theoretic features based on their importance, and the observation of these features is then used to focus attention accordingly.  Yet, often these networks are little more than diverse shadows on the wall.  They are not the mechanisms or the phenomena, they are not even observation: they are mere representation.
% 
% The structure of covert networks is highly variable from one context to another, and they change composition and interaction patterns over time.  With any covert network, there are strong constraints on passively and actively obtainable direct empirical data, overwhelming amounts of indirect data, and competing mechanical explanations for what is observed.  Studying the detailed history of any one network, or set of them, will not necessarily provide insight about how to effectively detect such networks in the future.  This situation recommends simulation of a variety of scenarios with accompanying detection strategies. Because of the censoring issues in the data, simulation requires particular attention to characterizing uncertainty and avoiding overfitting.
% 
% Herein, we discuss these issues in terms of network models.  Specifically, we review (i) incorporating observation uncertainty, (ii) the need for both back- and foreground populations, (iii) avoiding over-fitting, and (iv) some philosophy relative to implementation.
% 
% As part of that philosophical discussion, we walkthrough the exercise of procedurally generating back- and foreground populations, simulating communication among those individuals, and filtering those communications via an observation model.  We then address the issues of (i) fitting that model against data, and (ii) analyzing performance of the opposing sides (e.g. Receiver Operator Characteristic).  Finally, we also demonstrate a small model extensionand show how it affects the (unextended) detection performance.


\section*{Introduction}
For investigators ranging from anthropologists to law enforcement, the desire to find, study, and modify the behavior of groups operating at the edge of the observable is a principle concern.  In the meta-theatre of the ironic, various media outlets have responded to public interest by applying this lens to the activities of intelligence organizations world-wide -- themselves desperate to undertake this very task against other clandestine actors.  The revealation of these activities has sparked the symmetric concern: being able to operate clandestinely in an age of ubiquitous monitoring.  Criminal organizations have long appreciated the value of operating in secret, as have groups subject to State-sponsored abuse or industrial espionage, and time will tell whether or not this spark catches fire in general behavior.

The underlying drive for these opposed efforts is the implacably expanding byte trail.  Once, the recorded information of an entire life might have amounted only to a few bytes -- e.g., parish records on births and deaths, perhaps including morbidity.  Now information era tools produce a near endless supply of 1s and 0s.  In some cases, this production is a side effect of organized commercial instinct -- Google monetizing search through targeted advertisement, for example.  In other cases, individuals are voluntarily advancing their own brand -- Instagram ``selfies'', e.g.  However, in many cases this bitwise production is an engineering consequence of the associated technology.  E.g., cellular phones transmit constant location data, financial systems are increasing just digital ledgers substituting the certainty of physical exchange with sophisticated book-keeping to ensure accounts are square, and of course any use of the internet produces veritable bit contrails.  This rate of production far exceeds the personal processing capability of any practically-sized team of analysts.

Hence, these teams employ computer-based, heuristic filtration to decide which data to record, to review, and to obtain.  We avoid saying ``algorithmic'' at this point: ``algorithmic'' implies strong, logically inevitable conclusions from inputs.  Like ``algorithmic'' economic bubble prediction, we side more with Fama than Shiller.

Many such filters are based on social network descriptions.  Social networks are a compact way to represent measured relations in a population.  That compression, however, often involves irreversible aggregations: in strength -- e.g., co-employment as measured by working at the same university versus the same department versus in same the laboratory; in type -- e.g., engaging in commerce versus attending the same church versus having married relations; and in time -- e.g., co-appearance at a coffeeshop months ago versus attending classes together on a weekly schedule versus living together.

The mathematics may provide a formally guaranteed result from some particular input, but the more aggregation that occurs translating observations -- themselves imperfect snapshots of actual phenomena -- to representation, the more convincing evidence that must be provided before we should accept that those representations (and subsequent results from manipulating them) have meaning.

Given the real uncertainty, what these filters call for is testing and validation, but those present their own difficulties.  Calling field testing ``problematic'' seems like a gross understatement; reference ``truth'' ranges from incomplete to deceptive, and experiments could have dangerous side effects.  Even making use of intensely studied historical events is problematic: these offer no way to consider evolutionary behavior and technological innovation (without an additional layer of abstraction that would create an even more complex empirical challenge), even assuming the historical data are more than victor's embellishment.

Generating synthetic data seems like an obvious alternative.  It allows for comparison across both detection and masking strategies, consideration of multiple background contexts, forecasting of risks and tradeoffs in a way that allows uncertainties, and in general providing a framework for imaginative assessment.  Like all such flexible tools producing quantitative results, it has the subtle downside of the simulator's biases being validated by numerical gospel; if one believes a particular strategy is effective -- perhaps even with reasonable agreement for a particular time and situation -- there would be a natural tendency to ``adjust'' scenario parameters until they indicated the success of strategy.  Simple models are equally problematic, as they replace the parameter fudge-factor with the complementary assumptions fudge-factors.  Thus, we must always be skeptical - of hordes of purely fitted parameters that do not have an attached mechanism we can validate the resulting numbers against, and of the assumptions buried simple models that may in fact be driving model outcomes.

A messy business all around, and we have not even gotten to the technically hard details.

\section*{Skeptical Simulation}
If, as we believe, generating synthetic data for the purpose of testing covert SNA methods is the only pragmatic option, how should one proceed in light of all the obstacles?

As the point is to assess the conclusions of those methods against what is ``actually'' happening ({\em in silico}), on one end the process must obviously produce output that can be ultimately represented as a social network.  Since any the subject SNA method should include the process of converting real observations to the represention, the output should be in the form of those observations.  That part is conceptually straightforward, if not necessarily technically so.

The other end -- what is ``actually'' happening, at least in simulation -- is less conceptually straightforward, the problem being that we do not know all of what is actually happening.  We propose here that researchers adopt a modeling philosophy that makes them open to criticism.  That is, the model meets the following criteria:
\begin{itemize}
\item it is clear which parts are explicit (subject to fitting -- e.g., rate constants) versus implicit (assumptions -- e.g., interaction forms)
\item it is clear what those explicit and implicit processes are doing
\item it is clear where parts interact
\item it is clear how the observation process interacts with the modeled system
\end{itemize}

Consider the covert SNA described by Bright, et al. of the social networks created by judge's sentencing comments indicating personal association in methamphetamine production enterprises.  They provide the key components for the proposed modeling philosophy:
\begin{itemize}
\item a description of the subject process - e.g., acquisition of precursor, production in clandestine laboratory, distribution, as well as necessity of specialists
\item a description of how that process is observed - e.g., law enforcement investigation and subsequent prosecution
\item how the observations are flattened into a network representation - translation of judge's sentencing comments
\item the specific SNA methods applied and desired inferences - e.g., various centralities and applications to intervention
\end{itemize}

With the exception of the actual SNA, however, these are ``talking'' models.  We assert that to genuinely assess if the SNA based on judicial sentencing notes, we must explicitly understand how gang activity ultimately percolates into that ink.  That it is necessary to turn the descriptive models into computational ones, while preserving (and hopefully enhancing) our ability to argue with each piece of that story and all the between-the-lines implications.

\section*{Features of Good Computational Model}
What makes a good computational model?  Modellers in the traditional hard sciences (physics, chemistry, biology, and assorted sub disciplines) have historically emphasized purely computational concerns, which is quite sensible for problems that have simpler conceptual models (i.e., those that are mathematically formalizable) but relatively intense numerical tasks.  On the other hand, many non-academic software products deal with a quite sophisticated conceptual model with relatively simple numerical tasks, and that demand tends to push their development more towards programming and engineering concerns.  We assert this domain -- software engineering -- is the better reference for social science modeling, and that the lessons of that discipline are more keenly relevant (though they are certainly relevant to the natural sciences, especially as the biological sciences deal with more of the heterogeniety intrinsics to that domain).

We will emphasize a few key lessons, some ``ancient'' wisdom (two plus decades old) and some a bit more recent.  Good modeling is quite like good (scientific) story telling, with the exception that the story must be told by the code (though we include in ``code'' more than just what runs a particular simulation).

The overall goal is to make implementation choices that make the model itself communicate with other researchers.  This means the right level of dividing up the model.  Too little, and the rest of us have to comprehend the whole thing to comprehend a single atom of the story.  Too much, and instead we have to go to quarks to understand an atom of the story.

This begins with choosing a setup (language, libraries, frameworks, etc.) that make this possible.


\begin{itemize}
\item literate programming
\item design patterns - particularly emphasis on composition, decoration, adaptation over inheritance + achieving that with chain of responsibility 
\end{itemize}


They provide a descriptive model of drugs production that appears nearly identical to most industrial enterprises: acquisition of materials, production, retailing, wholesaling, and regulatory ``compliance'' (bribing officials).  Notably, the detail of this description and its quite reasonable analogy to regular enterprise.

There have been other attempts at such an approach\cite{carley2003dynamic}.

In the following sections, we lay out the uses and abuses of such a framework.  What makes for useful synthetic data sets?  What are the appropriate measures for detection strategies on them?  We motivate that discussion by inspiration from an over-simplified, network-based model of terrorism -- a sub-group of the Salafi jihad networks as described by Sageman {\em et al.}\cite{sageman} -- and community organization and communication.  Whether or not that work is an accurate description is not of particular concern.  Their qualitative properties are enough of a defensible testbed for modeling the communication patterns of one covert group.  We will point out where assumptions can be modified to identify different kinds of groups against a background population, since the behavior and structure of both the background and covert organizations are constantly evolving. 

\section*{Overview of the Simulation Problem}
Detecting a covert group is fundamentally about distinguishing the trace observable activity of that group from the activity of the background, bootstrapping that into guesses about the structure of the group, focusing the observation and distinguishing process based on that structure, and so on iteratively.  The purpose of simulation is ultimately to measure performance in the observer-covert group competition.

Given that these aspects must be expressed in the model (with varying degrees of detail), and that this model is used by a team or communicated between researchers, the language for that expression must be both powerful (to capture complexity where necessary), but also comprehensible to communicate what in fact is the meaning of the universe in this model.  Because these simulations are fundamentally about individuals and their behavior, we largely frame our discussion in terms of individuals, their internal state, and their interaction with others.

We can of course think in different terms.  For example, we are considering a network model of the population and covert group embedded within it, with edges representing relationships between people.  We might think of those relationships as being external state and correspondingly represent those relationships as external state in the simulation.  E.g., we might represent the population as a graph rather than a bag of individuals, and in the simulation the edges belong to the graph object rather than individuals.  As long as we recognize the translation between these perspectives, the choice boils down to what is most pragmatic relative to the science and simulation.  If most of the questions answered by the simulation relate to graph measures -- e.g., calculating various centrality measures -- then the simulation-as-graph rather than simulation-as-individuals probably makes more sense.

\subsection*{Modeling the Covert Groups}
Relative to the covert group, the simulation must model how the group members act and interact.  Typically, that further entails modeling a few moving pieces: (i) the members internal state (including what other members they know and communicate with), (ii) how that internal state evolves relative to external forces or states, and (iii) what actions those members undertake based on their internal state (possibly in response to some specific external event).

\subsection*{Modeling the Background}
Like the covert group, the background population is a (larger) group of individuals that act and interact.  They have the same basic modeling requirements as the covert group, but they behave differently.  This difference may be as simple as being distinct relative to observation process -- e.g., their communications are more likely to be recorded -- or it may be more noticeable like having fundamentally more diverse contacts, taking different kinds of actions, etc.

An alternative representation for the background would be to model it as a continuous entity instead of individuals in a network.  This could be used to reduce the simulation size.  However, this model would still need to create activity data that was consistent (in format) to that created by the covert group, because of the simulation requirements for observation.

\subsection*{Modeling Activity}
Individuals create modeled activity based on their internal state.  These actions correspond to (i) the observable events of specific interest (e.g., calls, financial transactions), (ii) generally observable events (e.g., travel, work, religious observations), and (iii) interactions that, while not externally observable, play a role in the evolution of internal state in other individuals either directly or by modifying the state of the world.  These actions should occur at specific times in the simulation, at whatever resolution is appropriate to the model.

The covert group and background population should be different in their activity; either performing different activities or undertaking them at different rates or on different schedules.

\subsection*{The Observation Model}
Activity is understood by the observing entities by an imperfect process.  Depending on the situation modeled, that process might simply miss events.  Alternatively, the error could be misunderstanding the type or content of the activity, or between what individuals and interaction is occurring.  In general, the observing entities should be able to modify what is observed at what rates.  Also, the observation process may itself alter the state of the world -- i.e., the covert group may have the opportunity to respond to the observation process.

\section*{A Language Matching the Simulation Requirements: Scala}
Scala\cite{odersky2004overview} is an mixin-inheritance\cite{bracha1990mixin}, object-oriented, functional language, which also includes an message-passing based model for parallel computing\footnote{The actor model is transitioning from the initial implementation to that developed by the Akka project\cite{akka}.  We base our discussion on the Akka model.  The most important difference is the notion of contexts and their switching and stacking: with a switching and stacking, an actor alters how it responds to events.  This is a useful abstraction for representing evolving individual state.}.  Indeed, Scala is used under the hood for the network simulation framework NetLogo.

Why do these traits make Scala particularly effective for the simulation requirements described above?  

First, the covert group members and the background population share behavior (in the real and simulation sense), but are not necessarily simple extensions, which makes a mixin inheritance object system a natural fit for the model subjects.

Second, many interactions in an organization can be thought of as management directing labor to complete a task; in a functional paradigm, functions are themselves treated as objects.  This translates natural to sending a function object from one simulated individual (in a leadership position) to another (in a subordinate position).  Loosely adopting some of the other functional paradigms, as Scala does -- emphasis on immutability, principally -- provides practical advantages to concurrent programming, which is necessary for large scale simulations.  Additionally, the functional emphasis bakes in other programming constructs to do with collection processing (e.g., iteration, transformation, filtering) that are less convenient in earlier languages.

Finally, the message-passing framework is an exact analogy to an individual / agent / actor - based model.  Having this baked into the language makes it a feature more like control branches (e.g., \texttt{if}, \texttt{else}, \texttt{for}) or collections, than a more complicated syntax available only to advanced programmers (and often only via library support) rather than more phenomena- and model-focused scientists.

\section*{Practical Application}
The practical application of these ideas is -- for a simple simulation -- as straightforward as stated.

Using the Scala actor framework, we simply define how each agent in the system responds to events, including the observing entities, and how they generate new events.  We accomplish the first by appropriately defining {\texttt receive} and the handling of different {\texttt case}s of messages.  We accomplish the second by having actors know about other actors, and having their internal state determine if they send events to these actors at any particular time.

In considering the salafi network (see \figref{salafi_network}), we will observe a highly constrained set of activity: only communication, with all of that communication having been reduced to \texttt{Good} or \texttt{Bad}.  The communication connections between individuals will be static, therefore we may set the social network at the outset of the simulation.

\wantFig{salafi_network}{the salafi network}

In the code, we could define different kinds of actors -- the covert leader, covert followers, background individuals -- as different classes of simulation object.  However, we instead take advantage of the context switching in the newer Scala actor framework.  By approaching the simulation this way, we lay a good foundation for observing the dynamics of individuals -- transitioning from background to covert member to covert leader -- as well as the dynamics of relationship and group formation and dissolution.

One syntactical aside for the code the below sections: the Scala collections (like \texttt{Set}) define methods for adding and removing elements in terms of the natural mathematical operators (\texttt{+}, \texttt{-}) and for similar operations with groups of elements (\texttt{++}, \texttt{--}).  Similarly, the names for message senders (\texttt{sender}), changing context (\texttt{context become ...}), et cetera are all quite clear.  Given the informative, mixed-natural-and-mathematical-like syntax, we prefer actual code snippets to pseudo-code to illustrate simulation steps.  However, we have elided some declarations that enable the more informative syntax from the initial appearance in code.

\subsection*{The Covert Group}
First, we define the covert group \texttt{Receive} context:

\begin{code}
  def plotter(
    // a plotter has superiors, collaborators, and subordinates
    // these groups are initially empty
    superiors:Plotters = empty,
    collaborators:Plotters = empty,
    subordinates:Plotters = empty
    /*, ... other args */) : Receive = {
      // ... to-be-defined behavior
      case _ => // ignore unmatched messages
  }
\end{code}

We need to define how they adopt and evolve this context.  Since we are not studying the organizational dynamics with this simulation, we can be phenomenologically loose for this part, though this approach means we can add monitoring that activity as part of future detection scenarios with minimal coding.  The simulation runner will send a \texttt{Radicalize} message to the initial cabal, then a \texttt{Collaborate} message to the same group:

\begin{code}
  object Radicalize
  case class Collaborate(group:Plotters)

  def receive = {
    case Radicalize =>
      context become plotter
    case _ => // ignore other messages
  }
  
  def plotter(...) => {
    case Collaboration(_, group) =>
      context become plotter(collaborators = collaborators ++ (group - self))
    // ...
  }
\end{code}

These leaders then need to recruit subordinates.  In this particular simulation, we are using exactly the network from Sageman et al., so we will have the simulation runner inform each plotter whom they are recruiting with a \texttt{Subordinates} message.  The leads will then send \texttt{Recruit} messages to those individuals and they will adopt the plotter context.

\begin{code}
  case class Subordinates(recruits:Plotters)
  object Recruit

  def plotter(...) = {
    case Subordinates(_, recruits) =>
      recruits foreach { recruit =>
        recruit ! Recruit
      }
    // ... other cases
  }
  
  def receive = {
    case Recruit =>
      context become plotter(superiors = Set(sender))
    // ... other cases
  }
\end{code}

At this point, we have all the necessary messages to construct the given network.  Those messages, based on the labels in Sageman et al. are covered in \Appref{sage_network}.  However, we still need to model how the plotters actually plot.  We will assume that all plotting (i) originates with leadership, (ii) when intermediates receive direction, they work with their \texttt{collaborators} as well as delegating work, (iii) that directions occasionally result in feedback to higher-ups, and finally (iv) all of the ``work'' and direction is captured as passing \texttt{Bad} messages.

The simulation runs as a series of iterations, and during each time step a leader may initiate a new \texttt{Bad} message.  Other members of the covert group circulate \texttt{Bad} messages subsequent to receiving a \texttt{Bad} message themselves, with different probabilities for sending messages to superiors, collaborators, and subordinates.  Those probabilities do not depend on the origin of the received message (though this could be a useful mechanical tweak to the model).

Though those probabilities could be coded into the actors as part of their initialization, if they are set and adjusted by events then the actors (i) more accurrately reflect real behavior and (ii) the evolution of individuals occurs via a consistent simulation mechanism.  The former provides a lever for expert insight and empirical study.  The latter makes it easier to produce real-alike data -- i.e., simulation data that has the form of real observations.

In our simplified example, we do not consider how ``real'' events would translate into adjusting a covert member's behavior.  Instead, the simulation runner sends a message, giving us an entry point for later adjustment without frontloading too much model complexity.  This represents the final step for how the plotters evolve in this simulation, so the following repeats the previous snippets with some condensing modifications to the code.  The most notable one is making use of a \texttt{case class} to represent all of the parameters that form a plotter's state.

\begin{code}
package object simactor { // defs to make implementation more natural
  type Plotters = Set[ActorRef]
  def empty : Plotters = Set.empty
  def group(list:ActorRef*) : Plotters = list.toSet
  type Probability = Double
}

object SimActor { // defines common elements for SimActors
  // MESSAGES ABOUT INTERNAL STATE
  object Radicalize                           // become a plotter
  object Recruit                              // work for a plotter
  case class Collaborate(group:Plotters)      // work with other plotters
  case class Subordinates(recruits:Plotters)  // recruit subordinates
  
  object PType extends Enumeration { // plotter behavior probabilites
    type PType = Value
    val Initiate, // prob. of starting plot comms
        Report,   // prob. of reporting to superiors
    	  Collab,	  // prob. of working w/ peers
    	  Delegate  // prob. of delegating work
    		  = Value
  }
  case class UpdateProb(changes:Map[PType.Value,Probability])

  // MESSAGES ABOUT EXTERNAL ACTIVITY
  object Bad
  object Good

  case class PlotterState( // the plotter state description
    superiors:Plotters = empty,
    collaborators:Plotters = empty,
    subordinates:Plotters = empty,
    probs:Map[PType.Value,Probability] = PType.values.map { (_, 0.0) } toMap
    // default each plotting probability 0.0
  )
}

class SimActor extends Actor {
  import SimActor._
  
  def receive = {
    case Radicalize =>
      context become plotter()
    case Recruit =>
      context become plotter(PlotterState(superiors = group(sender)))
    case _ => // ignore other messages
  }
  
  def plotter(ps:PlotterState = PlotterState()) : Receive = {
    import ps.{copy => change, _}
    def evolve(newPs:PlotterState) = context become plotter(newPs)
    
    {
       case Collaborate(group) =>
         evolve(change(collaborators = collaborators ++ (group - self)))
       case Subordinates(recruits) =>
         recruits foreach { recruit => recruit ! Recruit }
         evolve(change(subordinates = subordinates ++ recruits))
       case Recruit =>
         evolve(change(superiors = superiors + sender))
       case UpdateProb(changes) =>
         evolve(change(probs = probs ++ changes))
       case _ => // ignore other messages
    }
  }
}
\end{code}

Now we address how the plotters plot: if they receive a \texttt{Bad} message from within the covert group, they have some probability of continuing plot activity with their peers, some probability of delegating some work to subordinates (if they have any), and some probability of reporting to their supervisors (again if they have any).  We will treat these activities as (i) independent, and (ii) the interactions with a particular group as a series of independent binomial trials.  We are not aware of any mechanical explanation justifying this model of interaction; indeed, it implies that, in a finite slice of time for an individual with finite resources, interactions that consume time and resources never interfere.  For a large interval of time, that might be reasonable -- but large intervals of time conflicts with assumption that actions stimulate responses in subsequent intervals.  Alternatively, we might infer that there is substantial variability in events (in terms of their duration and resource requirements) such that many interactions means small events, but this undermines the model assumption that these activities can be lumped together into a single category.

The upside of this ``ideal gas'' model of social activity is ease of implementation.  However, we can circumscribe the appearance of this assumption in our simulation, so that some future Van der Waals can easily clean up our mess later with a more detailed interaction model.

We will use this as the probability of {\em any} \texttt{Bad} interaction with those groups, and that in general their interaction with these groups is binomially distributed.  Therefore, if a particular plotter has $k$ collaborators (or supervisors, or subordinates), the probability of interacting with an individual in that group, $p_i$, is related to the probability of interacting with the group, $p_g$ -- which is the probability in the simulation state -- by:

$$
(1-p_i)^k = 1 - p_g \rightarrow p_i = 1-\sqrt[k]{1-p_g}
$$

% // form a clique from a vertex collection, col:
% def clique(col : Collection[V]) =
%     for ( (left, right) <- undirectedPairs(col) ) // for each unique, undirected pair in col:
%       left <~> right // form a bidirectional edge across the pair
%       
% def hierarchicalClique(col: Collection[V], size:Int) = 
%    val thisLvl : Collection[Collection[V]] =
%      // make the lowest level cliques 
%      for ( subGroup <- groupBySize(col, size) ) yield {
%        clique(subGroup)
%        subGroup
%      }
%    cliqueGroups(thisLvl, size)
%    
% @recursive def cliqueGroups(col: Collection[Collection[V]], size:Int) =
%   val thisLvl : Collection[Collection[V]] =
%     // gather the groups being made into a higher level cliques
%     for ( subGroups <- groupBySize(col, size) ) yield {
%       // clique a group of groups
%       for ( (leftGroup, rightGroup) <- undirectedPairs(subGroup) ) {
%         leftGroup.randomMember <~> rightGroup.randomMember
%       }
%       // merge this set of subgroups into a single new group
%       merge(subGroups)
%     }
%   // repeat with the cliqued cliques, unless everything has been connected
%   if (thisLvl.size != 1) cliqueGroups(thisLvl, size)  
% \end{code}
% 
% %To test strategies for either the covert group or survillience entity, one must have a model capable of representing their strategies.  That means modeling entities that take action, modeling how that action is observed (including if it is observed correctly, or at all), and modeling how those observations are digested into reactions.  Notably, the entities must include some sort of background -- if the only data being simulated is to do with the covert entities, they are hardly covert within that simulation.  Here, we will focus on network based models of these components.  Networks seem like a natural tool, given the role of individually-based action, discrete events, and the relatively small number of participants in these groups.  Though we do not do so in our example, the background population might be more tractably modeled with continuous phenomena, given its large size and potentially more homogeneous behavior.
% 
% %\subsection*{Modeling the Entities}
% %For our motivating example, we divide the population into three types, two of which belong to the covert group -- management and subordinates -- and a third representing ordinary individuals in the background population.  We choose this number of types because we are using that many models of activity, though different degree nodes will present somewhat differently.  The background population may be less homogenous in types, or perhaps types may be better modeled as being selected from distribution of features.  What must be guarded against here is over fitting by mechanism -- essentially the problem of choosing a polynomial of power equal to the available data, but more subtle.  Fishing with different mechanics may yield a better historical fit, but not necessarily a better forecast.
% 
% %\subsubsection*{Background Population}
% %Most observable action will be that of the general populace surrounding the clandestine group.  This population has some structural component -- {\em e.g.} family sizes, typical numbers of working members, tendency towards assortativity -- though not necessarily well-known when a given investigation begins and perhaps even assumed to be something it is not.  This structure is also likely dynamic due to natural evolution (or activity on the structure may change pattern dynamically, those perspectives not being easy to disentangle), or possibly in response to the investigation.  For example, the ongoing revelations about the NSA will no doubt influence the behavior of the technical elite and percolate into the general public.  Therefore, assessment of any particular pair of opposed strategies should cut across multiple models of the background, each independently parametrized around what data is available.
% 
% As an example background population, we have the ordinary individuals form small groups, which in turn connect into larger groups, those groups into still larger groups, and so on until the background consists of a single component.  If one were inclined to require that this description corresponded to a particular mechanism, this might loosely be interpreted as individuals forming households, households forming blocks, blocks forming neighborhoods, {\em ad nauseum}.  However, here it is only an academic fiction -- a compact, algorithmically and analytically convenient expression, without any connection to well-established mechanics or data.  If demographic data for households were available, then we could plausibly parameterize the lowest level, then possibly combine that with mortality and mobility data to characterize how closely connected households remained, and so on.
% 
% Independently, we establish a second set of edges with a different flavor.  The previously described edges we label ``Familial'', these we call ``Economic''.  We will generate these in an identical fashion.  Again, if one were inclined to propose an explanation, one might call these small businesses or groups within a business, those forming collaborating businesses or whole firms, and so on hierarchically.  Again, we emphasize: this choice is purely an academic fiction, where we have added this extra fiction purely to highlight the need for multiple dimensions to represent different kinds of relationships in the population.
% 
% For both of these types, the ``grouping'' operation is to try to form cliques of size $n$ (with allowance to handle an arbitrary total population size).  That is:\begin{enumerate}
% \item create and randomly permute a population $P_0$, with $N$ members (later results use $N=100$).
% \item\label{grouping_init} divide $P_0$ into equal groups of size $n$ (later results use $n=3$);
% \item for each group $i$, completely connect the individuals, and label that group $C_i^0$
% \item\label{grouping_end} form a population from the $C_i^0$
% \item repeat steps \ref{grouping_init} to \ref{grouping_end} with the $C_i^0$ connecting each edge between the $C_i^0$ to a uniformly drawn individual within the group, then with the $C_i^1$, etc until a single component is obtained
% \end{enumerate}
% 
% \begin{code}[title=Pseudo-Code: Hierarchical Cliques]
% // form a clique from a vertex collection, col:
% def clique(col : Collection[V]) =
%     for ( (left, right) <- undirectedPairs(col) ) // for each unique, undirected pair in col:
%       left <~> right // form a bidirectional edge across the pair
%       
% def hierarchicalClique(col: Collection[V], size:Int) = 
%    val thisLvl : Collection[Collection[V]] =
%      // make the lowest level cliques 
%      for ( subGroup <- groupBySize(col, size) ) yield {
%        clique(subGroup)
%        subGroup
%      }
%    cliqueGroups(thisLvl, size)
%    
% @recursive def cliqueGroups(col: Collection[Collection[V]], size:Int) =
%   val thisLvl : Collection[Collection[V]] =
%     // gather the groups being made into a higher level cliques
%     for ( subGroups <- groupBySize(col, size) ) yield {
%       // clique a group of groups
%       for ( (leftGroup, rightGroup) <- undirectedPairs(subGroup) ) {
%         leftGroup.randomMember <~> rightGroup.randomMember
%       }
%       // merge this set of subgroups into a single new group
%       merge(subGroups)
%     }
%   // repeat with the cliqued cliques, unless everything has been connected
%   if (thisLvl.size != 1) cliqueGroups(thisLvl, size)  
% \end{code}
% 
% Lastly for this model, we establish a final set of edges with a third flavor: ``Religious''.  These edges occur between members with a probability based the distance between the individuals on the ``Familial'' graph.  That is -- for those wanting to assign a meaning -- members of the same family are most likely to observably interact in a religious capacity, then immediate relatives or neighbors, and so on.  Of course, this is again academic, chosen to illustrate that other generation algorithms are possible, even with dependence between dimensions.  The detailed algorithm is:\begin{enumerate}
% \item\label{religious_init} for each individual $i$:
% \item assign $d_i=1$ and $F_i$ to the set of all their familial connections, excluding individuals already considered
% \item with probability $p^{d_i}$ connect $i$ to the members of $F_i$
% \item\label{religious_end} increment $d_i$, move all of $F_i$ to $P_i$, then add all of the familial connections of $P_i$ that are not in $P_i$ (or previously considered) to $F_i$.
% \item repeat steps \ref{religious_init} to \ref{religious_end} until $F_i$ has no members added in step \ref{religious_end} 
% \end{enumerate}
% 
% <<readInCliqued,echo=FALSE>>=
% library(igraph)
% pree <- read.table("../1-EL.txt",col.names=c("sender_id","recipient_id","type"))
% pree[which(pree[,1]<0),1] <- -pree[which(pree[,1]<0),1]
% pree[which(pree[,2]<0),2] <- -pree[which(pree[,2]<0),2]
% edgeinfo <- t(pree)
% # hack: doesn't seem reasonable to have sim output other vertex labels
% #edgeinfo[1,which(edgeinfo[1,] == 107)] <- 103
% #edgeinfo[2,which(edgeinfo[2,] == 107)] <- 103
% #edgeinfo[1,which(edgeinfo[1,] == 112)] <- 104
% #edgeinfo[2,which(edgeinfo[2,] == 112)] <- 104
% 
% gWhole <- graph(edgeinfo[-3,])
% V(gWhole)$size<-2
% V(gWhole)[c(102,103,104)]$size<-6
% E(gWhole)[which(edgeinfo[3,]=="Work")]$color <- "green"
% E(gWhole)[which(edgeinfo[3,]=="Work")]$weight <- 0.1
% E(gWhole)[which(edgeinfo[3,]=="Family")]$color <- "blue"
% E(gWhole)[which(edgeinfo[3,]=="Family")]$weight <- 0.1
% E(gWhole)[which(edgeinfo[3,]=="Religion")]$color <- "purple"
% E(gWhole)[which(edgeinfo[3,]=="Plot")]$color <- "red"
% #gAlt <- delete.vertices(gWhole, which(degree(gWhole) < 1)-1)
% l<-layout.auto(gWhole)
% @
% 
% \begin{rnwfig}
% <<all, fig=TRUE, width=7.5, echo=FALSE>>=
% plot(gWhole, edge.arrow.size=0.01, edge.color="grey", vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{Overlay of all connections; the large vertex represents the cluster of subordinates.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<religion, fig=TRUE, width=7.5, echo=FALSE>>=
% E(gWhole)[which(edgeinfo[3,]=="Work")]$color <- NA
% E(gWhole)[which(edgeinfo[3,]=="Family")]$color <- NA
% plot(gWhole, edge.arrow.size=0.01, vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{The same graph, showing the plot connections (red) and religious (purple) links.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<work, fig=TRUE, width=7.5, echo=FALSE>>=
% E(gWhole)[which(edgeinfo[3,]=="Work")]$color <- "green"
% E(gWhole)[which(edgeinfo[3,]=="Religion")]$color <- NA
% plot(gWhole, edge.arrow.size=0.01, vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{Now showing work (green) links.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<family, fig=TRUE, width=7.5, echo=FALSE>>=
% E(gWhole)[which(edgeinfo[3,]=="Family")]$color <- "blue"
% E(gWhole)[which(edgeinfo[3,]=="Work")]$color <- NA
% plot(gWhole, edge.arrow.size=0.01, vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{Now showing family (blue) links.}
% \end{rnwfig}
% 
% We also consider an alternative background: instead of cliques for ``Familial'' and ``Economic'' ties, individuals form trees.  The treatment for ``Religious'' ties remains the same.  The trees have a similar $n$ parameter for branching, and form bidirectional links.  This is obviously less sophisticated than the aforementioned hierarchical structures.  Is it less plausible as well?  Certainly, it seems off relative to, say, a NATO country: no interaction loops in a particular social context seems unimaginable.  Perhaps this is more reasonable in a setting where there is less information technology penetration or less observable interaction.  Perhaps its more plausible if the trees instead had siblings form cliques as well.  None of that supposition is particularly relevant: we picked another academic fiction to compare with our previous one.
% 
% <<readInTree,echo=FALSE>>=
% pree <- read.table("../1-EL-alt.txt",col.names=c("sender_id","recipient_id","type"))
% pree[which(pree[,1]<0),1] <- -pree[which(pree[,1]<0),1]
% pree[which(pree[,2]<0),2] <- -pree[which(pree[,2]<0),2]
% edgeinfo <- t(pree)
% 
% gWholeAlt <- graph(edgeinfo[-3,])
% V(gWholeAlt)$size<-2
% V(gWholeAlt)[c(102,103,104)]$size<-6
% E(gWholeAlt)[which(edgeinfo[3,]=="Work")]$color <- "green"
% E(gWholeAlt)[which(edgeinfo[3,]=="Work")]$weight <- 0.1
% E(gWholeAlt)[which(edgeinfo[3,]=="Family")]$color <- "blue"
% E(gWholeAlt)[which(edgeinfo[3,]=="Family")]$weight <- 0.1
% E(gWholeAlt)[which(edgeinfo[3,]=="Religion")]$color <- "purple"
% E(gWholeAlt)[which(edgeinfo[3,]=="Plot")]$color <- "red"
% l<-layout.auto(gWholeAlt)
% @
% 
% \begin{rnwfig}
% <<allAlt, fig=TRUE, width=7.5, echo=FALSE>>=
% plot(gWholeAlt, edge.arrow.size=0.01, edge.color="grey", vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{Overlay of all connections; again, the large vertex represents the cluster of subordinates.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<religionAlt, fig=TRUE, width=7.5, echo=FALSE>>=
% E(gWholeAlt)[which(edgeinfo[3,]=="Work")]$color <- NA
% E(gWholeAlt)[which(edgeinfo[3,]=="Family")]$color <- NA
% plot(gWholeAlt, edge.arrow.size=0.01, vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{The same graph, showing the plot connections (red) and religious (purple) links.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<workAlt, fig=TRUE, width=7.5, echo=FALSE>>=
% E(gWholeAlt)[which(edgeinfo[3,]=="Work")]$color <- "green"
% E(gWholeAlt)[which(edgeinfo[3,]=="Religion")]$color <- NA
% plot(gWholeAlt, edge.arrow.size=0.01, vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{Now showing work (green) links.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<familyAlt, fig=TRUE, width=7.5, echo=FALSE>>=
% E(gWholeAlt)[which(edgeinfo[3,]=="Family")]$color <- "blue"
% E(gWholeAlt)[which(edgeinfo[3,]=="Work")]$color <- NA
% plot(gWholeAlt, edge.arrow.size=0.01, vertex.label=NA, vertex.frame.color=NA, layout=l)
% @
% \caption{Now showing family (blue) links.}
% \end{rnwfig}
% 
% \subsubsection*{Covert Actors}
% Sageman, Qin, et al. describe the structure of the Salafi networks as comprising a few key individuals with links to a large group of lieutenants -- the middle management of terror -- that are each connected to several tightly clustered subordinate groups -- terrorist cells -- that execute plots.  The lieutenants typically integrate with the regular population, while the subordinate groups are largely cloistered.
% 
% So we consider a covert organization consisting of two types, those directing a plot, {\em Management}, and those carrying out the day-to-day details, {\em Subordinates}.  For this demonstration model, we consider a ``small enough'' plot with a ``narrow enough'' schedule, such that only a single plot will occur during the simulation and that only a single manager is necessary to run that plot.
% 
% This manager, $M$, exists along side the background population.  Binomially sample the $C_i^0$ for ``Familial'' (with probability $p_F$) and ``Economic'' (with probability $p_E$) graphs and add $M$ to the selected $C_i^0$.  Then create ``Religious'' edges according to the same procedure used by ordinary individuals.
% 
% The subordinates, however, are isolated from the background population.  The manager and subordinates form a clique with a new type of edge: ``Plot''.
% 
% We also consider an alternative model: $M$ joins the background population as described above, but connects to the subordinate group via a hierarchical tree.
% 
% \subsection*{Modeling Action \&\ Observation}
% Our proposed types have differences in their structural organization, but we also use those types to distinguish activity by those types on their related structure.  In this assessment, we represent activity only as monitorable communications, and those communications have their content flattened into two categories: ``Good'' and ``Bad''.  This is obviously a gross simplification of individual behavior (or over-estimation of analyst categorization capabilities); a potentially more appropriate version would be to have an abstract vocabulary with usage distinctions between the background and the clandestine group (e.g., uniform use in the background versus enriched in a subset in the clandestine group).  However, as we no doubt boringly emphasize: there is no particular basis for informing this model.  A time and group sensitive partitioning of intercepts for variety and distribution could plausibly form a basis for such a fit; one would have to consider, however, the distinction between the open source background communications (i.e., generally known to be public) versus the intercepted communications of the clandestine group (generally assumed private).
% 
% One last technical issue before proceeding to our example implementation: activity models may obviously differ in the data they generate, both structurally and semantically.  The underlying masking and investigating strategies may be framed relative to a particular action model, which may then require an adaptation of the activity data to be consistent with those adjacent models.
% 
% \subsubsection*{Background Action}
% Each simulation time step, individuals in the background population generate messages by binomially sampling from all of their available connections.  They exhibit no preference for the type of those connections (beyond the structural consequence of having different numbers of different types).  If one interprets each message as a whole conversation, initiated by the sending party, then one implication of this model is that their messaging activity occupies an inconsequential period of real time relative to the real time equivalent of simulation step.  If one believed it was useful to model conversations explicitly -- i.e., each message is a word or phrase -- then one obvious change would be that individuals would only be selecting one target at a time, as well as modeling the speaker switch versus terminating communication.  This highlights a previously mentioned problem for the observation strategy -- there must exist an adapter for strategies between data types.  In this proposed continuous model, the aggregation for a strategy that only considers whole messages might be to average (on some specific real time scale) the total vocabulary in each direction of the conversation and then send one message from each party.
% 
% As for message content, the background population sends ``Good'' messages with a higher probability than ``Bad'' messages.
% 
% \subsubsection*{Covert Action}
% The clandestine group's manager, $M$, behaves much like background population relative to his non-``Plot'' edges.  His tendency to send ``Bad'' messages to the background population is defined relative to the background probability.
% 
% However, he additionally sends direction to the subordinate group via the ``Plot'' connections with some probability.  Presumably, he would balance execution rate with secrecy.
% 
% The subordinates, however, do not interact with the background population.  They randomly speak with each other, with an enhanced probability of sending ``Bad'' messages after having received them from another member of the clandestine group.
% 
% \subsection*{Modeling Observation}
% Incomplete information is the norm in these investigations, much like any work in the non-physical sciences.  However, it is typically possible to modulate what information is available (by investing more resources, by redirecting assets, etc).  We do not illustrate any strategies for either side in terms of modulating the flow of information, though an obvious addition dimension to add to both parties would be some resource pool that can be applied to modifying what information arrives at the observer (e.g., modulating the probability of detecting ``Bad'' messages from the covert group, faking messages in the background or to suspect members of the clandestine group).  For our simple model, we have different detection rates for the various edge flavors: ``Economic'' being highest, then ``Religious'', then ''Familial'', then ''Plot''.  The type of edge used to transmit the message is also not disclosed.
% 
% \subsection*{Modeling Reaction}
% We rate investigating entities that implement a few different strategies.  One strategy is purely structural based on degrees, another purely content-oriented based on ``Bad'' frequency, and the third mixes the first two.
% 
% \subsubsection*{Structural Strategy}
% Structural identification strategies range from having strong prior belief about a particular feature -- e.g., degree distribution -- and a relatively simple detection computation to belief only that there is a meaningful structural distinction (and a covert group to detect at all), thus running up against practical computational considerations trying to analyze all possible distinctions.
% 
% We demonstrate a case of the former, positing that the unique structural feature is to do with degree distribution -- specifically, a relatively high degree distribution for the manager, a relatively low degree distribution for the subordinates, and connection between them.  The criteria for labeling a member of the covert group is then a matter of setting what slices of the distribution to take from the top and bottom, and then testing for an observed path between the manager and the subordinate.
% 
% \subsubsection*{Content Strategy}
% A pure content strategy ignores details about sender and receiver arrangement, instead focusing on the sending and receiving of different types of messages.  For our simple model, we consider a strategy that measures relative in and outflow of messages and the frequency of ``Bad'' messages.
% 
% \subsubsection*{Combination Strategy}
% For our example combination strategy, we simply require individuals pass both threshold measures.
% 
% \section*{Evaluation}
% There are two basic aspects of evaluation, which correspond to the general scientific method questions of model fitting versus model selection.  Note that these are entirely separate from detailed application of these aspects to particular model components; those activities are certainly critical in narrow assessments, but we must again emphasize the ability of both sides to adapt their strategies, improve technological capabilities, and so on, all of which will present disruptive changes to any established model.
% 
% The aspect which corresponds to questions of fit is basically identifying, for a particular model context -- specific combination of opposed strategies and background activity -- parameter surfaces for optimal performance on the chosen metrics.
% 
% The question of model selection corresponds more to considering these performance surfaces across a wide breadth of combinations.  That is, how well does a particular covert strategy perform across multiple background population behaviors and against varying investigatory capabilities?  Vice versa for the investigatory system?
% 
% For our toy systems we consider a simple performance metric, the Receiver Operator Characteristic discrimination statistic, across a background parameter sweep and as a function of time duration. 
% 
% <<preRes,echo=FALSE>>=
% structureFiles <- sort(list.files("..","structure_\\d+\\.txt"))
% contentFiles <- sort(list.files("..","content_\\d+\\.txt"))
% sandcFiles <- sort(list.files("..","sandc_\\d+\\.txt"))
% badFiles <- sort(list.files("..","bad_\\d+\\.txt"))
% astructureFiles <- sort(list.files("..","astructure_\\d+\\.txt"))
% acontentFiles <- sort(list.files("..","acontent_\\d+\\.txt"))
% asandcFiles <- sort(list.files("..","asandc_\\d+\\.txt"))
% abadFiles <- sort(list.files("..","abad_\\d+\\.txt"))
% popsize <- 100
% plotsize <- 6
% 
% qData<-function(src) {
%   FPR<-sapply(src,function(fname) {
%     read.table(paste("../",fname,sep=""),header=F,sep=" ",col.names=c("FPR","TPR"))$FPR
%   },simplify="matrix") / popsize
%   FPRq <- apply(FPR,1,quantile)
%   TPR<-sapply(src,function(fname) {
%     read.table(paste("../",fname,sep=""),header=F,sep=" ",col.names=c("FPR","TPR"))$TPR
%   },simplify="matrix") / plotsize
%   TPRq <- apply(TPR,1,quantile)
%   list(TPRq=TPRq,FPRq=FPRq)
% }
% sQData <- qData(structureFiles)
% sFPRq <- sQData$FPRq
% sTPRq <- sQData$TPRq
% asQData <- qData(astructureFiles)
% asFPRq <- asQData$FPRq
% asTPRq <- asQData$TPRq
% 
% cQData <- qData(contentFiles)
% cFPRq <- cQData$FPRq
% cTPRq <- cQData$TPRq
% acQData <- qData(acontentFiles)
% acFPRq <- acQData$FPRq
% acTPRq <- acQData$TPRq
% 
% sandcQData <- qData(sandcFiles)
% sandcFPRq <- sandcQData$FPRq
% sandcTPRq <- sandcQData$TPRq
% asandcQData <- qData(asandcFiles)
% asandcFPRq <- asandcQData$FPRq
% asandcTPRq <- asandcQData$TPRq
% 
% badMR <- sapply(badFiles,function(fname) {
%   read.table(paste("../",fname,sep=""),header=F,col.names=c("badMR"))$badMR
% },simplify="matrix")
% badMR <- badMR / max(badMR)
% badMRq <- apply(badMR,1,quantile)
% 
% abadMR <- sapply(abadFiles,function(fname) {
%   read.table(paste("../",fname,sep=""),header=F,col.names=c("badMR"))$badMR
% },simplify="matrix")
% abadMR <- abadMR / max(abadMR)
% abadMRq <- apply(abadMR,1,quantile)
% 
% timesteps <- 100
% samples <- 100
% t<-1:timesteps
% tm <- matrix(t,nrow=timesteps,ncol=samples)
% backpoly<-function(t,x,color,lb=0) {
%   polygon( c(min(t), t, max(t)), c( lb, x, lb), col=color, border=NA )
% }
% liner<-function(t,TPRq,FPRq,TPRxy,FPRxy) {
%   lines(x=t,y=TPRq[3,],col="green3",type="l",xlab="iterate",ylab="FPR",ylim=c(0,1))
%   lines(x=t,y=TPRq[4,],col="darkolivegreen1")
%   lines(x=t,y=TPRq[2,],col="darkolivegreen1")
%   lines(x=t,y=FPRq[3,],col="red")
%   lines(x=t,y=FPRq[4,],col="coral")
%   lines(x=t,y=FPRq[2,],col="coral")
%   text(TPRxy[1],TPRxy[2],"TPR",col="green3")
%   text(FPRxy[1],FPRxy[2],"FPR",col="red")
% }
% plotter<-function(t,badMRq,TPRq,FPRq,TPRxy,FPRxy) {
%   plot(t,badMRq[5,],type="l",col="grey93",ylab="",xlab="",yaxt="n",xaxt="n",bty="n")
%   backpoly(t,badMRq[5,],col="grey93")
%   backpoly(t,badMRq[4,],col="grey85")
%   backpoly(t,badMRq[2,],col="grey93")
%   backpoly(t,badMRq[1,],col="white",lb=-1)
%   axis(side=1,at=c(min(t),max(t)),labels=c(expression(t[min]),expression(t[max])),tick=F,line=-1)
%   text(75,0.5,"MR")
%   liner(t,TPRq,FPRq,TPRxy,FPRxy)
% }
% @
% \subsection*{Structural Performance}
% \begin{rnwfig}
% <<structure, fig=TRUE, width=7.5, echo=FALSE>>=
% plotter(t,badMRq,sTPRq,sFPRq,c(20,0.5),c(80,.15))
% @
% \caption{Structure-Only performance versus clique-based background.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<astructure, fig=TRUE, width=7.5, echo=FALSE>>=
% plotter(t,abadMRq,asTPRq,asFPRq,c(20,0.5),c(80,.15))
% @
% \caption{Structure-Only performance versus tree-based background.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<content, fig=TRUE, width=7.5, echo=FALSE>>=
% plotter(t,badMRq,cTPRq,cFPRq,c(20,0.5),c(80,.15))
% @
% \caption{Content-Only performance versus clique-based background.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<acontent, fig=TRUE, width=7.5, echo=FALSE>>=
% plotter(t,abadMRq,acTPRq,acFPRq,c(20,0.5),c(80,.15))
% @
% \caption{Content-Only performance versus tree-based background.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<sandcontent, fig=TRUE, width=7.5, echo=FALSE>>=
% plotter(t,badMRq,sandcTPRq,sandcFPRq,c(20,0.5),c(80,.15))
% @
% \caption{Structure and Content performance versus clique-based background.}
% \end{rnwfig}
% 
% \begin{rnwfig}
% <<asandcontent, fig=TRUE, width=7.5, echo=FALSE>>=
% plotter(t,abadMRq,asandcTPRq,asandcFPRq,c(20,0.5),c(80,.15))
% @
% \caption{Structure and Content performance versus clique-based background.}
% \end{rnwfig}
% 
% \section*{Discussion}
% \todoCP{Get figures to give some direction for this.  Probably going to see the interesting stuff in the differently organized pops.}

\bibliography{manuscript}{}
\bibliographystyle{plain}

\begin{appendices}
\section{Code for Base Simulation Actor}

\section{Messages to Construct Salafi Network}\applabel{sage_network}
\begin{code}
  // ...TODO figure out these messages
  val cabal = Set(``A'',``B'',``C'');
  cabal foreach { _ ! Radicalize(...) }
  cabal foreach { _ ! Collaboration(..., cabal) } 
\end{code}

\end{appendices}

\end{document}