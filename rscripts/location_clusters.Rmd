---
output:
    pdf_document:
        includes:
            in_header: header.tex
        fig_caption: yes
---
## Analysis and plots of montreal data.

```{r, loadpkgs, echo=FALSE, include=FALSE}
req.packages <- c("data.table", "ggplot2", "reshape2", "grid", "scales", "lubridate", "movMF")
need.pkgs <- req.packages[!sapply(req.packages, require, character.only = T)]
if (length(need.pkgs) != 0) install.packages(need.pkgs, dependencies = T)
source("munging.R")
source("pca_plots.R")
```

### Required packages: `r req.packages`.

Source data:

```{r, loadData, echo=FALSE}
max_hours <- 24
min_logins <- 2;    min_users <- 6 # 6 user min remove abberant location
min_lifetime <- 30; min_loc_lifetime <- 30 # days
censor.dt <- loadCensored(
  raw.dt = loadRaw(),
  max_hours = max_hours, min_logins = min_logins, min_lifetime = min_lifetime,
  min_users = min_users, min_loc_lifetime = min_loc_lifetime
)
censor.dt
set.seed(0); training_portion <- .8
unique_locations <- censor.dt[,unique(location_id)]
training_locations <- sort(sample(
  unique_locations,
  size = length(unique_locations)*training_portion, replace = F
))
# ggplot(censor.dt[,list(usage=sum(logout-login)/60/60), by=list(location_id, year, month)]) + aes(x=month, y=usage, group=location_id) + facet_grid(year ~ .) + geom_line(alpha=0.5)
# users that qualify as regular:
# censor.dt[,list(lifetime = max(logout)-min(login)), by=list(location_id, user_id)][lifetime > 60*60*24*365, unique(user_id)]
```

Let's look at some quick views of some components of the data.  First, how many hotspots are in use?  We can look at the life of all hotspots:

```{r, location_counts, echo=F, fig.cap="Location Counts"}
loclifetimes <- censor.dt[,
  list(start=min(login), end=max(logout)),
  keyby=location_id
]
p <- ggplot(loclifetimes) + theme_bw() + aes(y=location_id, yend=location_id, x=start/3600/24, xend=end/3600/24) + geom_segment(alpha=0.5) + labs(x="start / end day", y="location") + scale_x_continuous(breaks=(0:6)*365)
ggsave("location-liftimes.png", p, width=7, height=4)
p
```

as well as the net count of locations over time:

```{r, location_counts, echo=F, fig.cap="Location Counts"}
locCounts <- melt.data.table(loclifetimes, id.vars = "location_id")[,
  list(change=sum(c(1,-1)[variable])), keyby=value
]
p <- ggplot(locCounts[,list(time=value/3600/24, net=cumsum(change))][net>0]) + theme_bw() + aes(x=time, y=net) + geom_step() + coord_cartesian(xlim=c(1,2000), ylim=c(1,300), expand = F) + scale_y_log10("net locations", breaks=c(1,2,5,10,20,50,100,200)) + scale_x_continuous(breaks=(0:6)*365)
ggsave("location-net.png", p, width=7, height=4)
p
```

There's a similar trend in users:

```{r, location_counts, echo=F, fig.cap="Location Counts"}
userCounts <- melt.data.table(lifetimes <- censor.dt[,
  list(start=min(login), end=max(logout)),
  keyby=user_id
], id.vars = "user_id")[,
  list(change=sum(c(1,-1)[variable])), keyby=value
]
p <- ggplot(userCounts[,list(time=value/3600/24, net=cumsum(change))][net>0]) + theme_bw() + aes(x=time, y=net) + geom_step() + coord_cartesian(xlim=c(1,2000), ylim=c(1,20000), expand = F) + scale_x_continuous(breaks=(0:6)*365) + scale_y_log10("net users", breaks=c(1,2,5,10,20,50,100,200,500,1000,2000,5000,10000,20000))
ggsave("user-net.png", p, width=7, height=4)
p
```

If want to identify locations which are "similar", so that we can inform how the covert group makes use
of locations, but we have no particular information about those locations (*e.g.*, what businesses they are, where they are geographically), what are we to do?

This question falls neatly into the sort of problems described as "unsupervised learning", so we will combine approaches from that field with "analyst discretion".  Having to rely on domain expert input here is largely unavoidable, but we can avoid some of the traps associated with that (more sophosticated, perhaps, but essentially 'p-hacking') by cross-validating the outcomes at different stages.

TODO: what stages? how? definitely need at detection stage: can we get "no detect" when making non-covert synthetic people?

Initially, we are going to trawl the data, sieving out a great variety of measurements for locations, and progressively identify which to maintain, discard, or merge, attempting to find a compelling argument as to why.  While these specific choices are unlikely to apply to another data source, we think the process will be illustrative as to what an analyst in the field should do.

Our initial measures are:

| measure | reasoning | transform? |
|--------:|-----------|-----------:|
| total lifetime | other parameters are likely normalized by this | log |
| total usage | low vs. high use locations is obvious distinction | log |
| number of unique users | few vs. many unique visitors indicates location "population" | log |
| quartiles for login durations | typically short vs. long logins indicates turnover | none |
| mean, sd of login durations | ibid. | none |
| % of usage by week day | weekend vs working vs continuous availability | none |
| % of usage by month | seasonal vs non | none |
| % of log(in\|out), usage by hour | working day vs evening use | none |

This is 101 (!) measures, so we definitely need to reduce the dimensionality.

```{r, echo=F}
day.abb <- c("Sun", "Mon", "Tues", "Wed","Thurs","Fri","Sat")

weekday_zeroes <- data.table(
  weekday=factor(day.abb, levels = day.abb, ordered = T),
  usage = 0, key = "weekday"
)

monthly_zeroes <- data.table(
  month=factor(month.abb, levels = month.abb, ordered = T),
  usage = 0, key = "month"
)

hourly_zeros <- data.table(
  hour = 0:23,
  usage = 0, key = "hour"
)

process_hour_rle <- function(prefix) {
  hrsind <- 0:23
  nms <- sapply(hrsind, function(h) sprintf("%s_hr_%.2d", prefix, h))
  function(hrrle) with(hrrle, {
    missing <- hrsind[!(hrsind %in% values)]
    res <- c(lengths, rep.int(0, length(missing)))[order(c(values, missing))] / sum(lengths)
    names(res) <- nms
    res
  })
}

login_parser <- process_hour_rle("login")
logout_parser <- process_hour_rle("logout")

with_usage_zeros <- function(orig.usage, zeros) orig.usage[zeros][,
  list(usage = ifelse(is.na(usage), i.usage, usage)), keyby=key(zeros)
]

refpredictors <- function(SD) with(SD, {
  duration <- logout - login
  total_login_time <- sum(duration)
  usage_by_weekday <- with_usage_zeros(SD[,
    list(usage = sum(login_day_secs)), keyby=weekday
  ], weekday_zeroes)
  usage_on_logout_day <- with_usage_zeros(SD[,
    list(usage = sum(logout_day_secs)), keyby=weekday
  ], weekday_zeroes)$usage
  
  weekday_percent <- with(usage_by_weekday, {
    ## logout usage "on" Sunday belongs to Monday, on M to Tues, etc
    usage <- (usage + c(usage_on_logout_day[7], usage_on_logout_day[-7])) / total_login_time
    names(usage) <- weekday
    usage
  })
  
  usage_by_month <- with_usage_zeros(SD[,
    list(usage = sum(logout - login) / total_login_time), keyby=month
  ], monthly_zeroes)

  monthly_percent <- with(usage_by_month, {
    names(usage) <- month
    usage
  })
  
  user_count <- length(unique(user_id))
  life <- max(logout) - min(login)

  dur_quantiles <- quantile(duration)
  login_hours <- login_parser(rle(sort(login_hour)))
  logout_hours <- logout_parser(rle(sort(logout_hour)))
    
  short <- (login_hour == logout_hour) & (login_day == logout_day)
  
  login_usage_by_hour <- data.table(usage = 
      ifelse(short, logout_time - login_time, (login_hour + 1)*3600 - login_time), hour = login_hour)
  logout_usage_by_hour <- data.table(usage = 
      logout_time[!short] - logout_hour[!short]*3600, hour = logout_hour[!short])
  
  long <- (login_day != logout_day) & !(login_hour == 23 & logout_hour == 0)
  cross_day <- rbindlist(with(SD[long, list(login_hour, logout_hour)],
    mapply(function(in_hr, out_hr) {
      data.table(usage = rep.int(3600, out_hr-in_hr+23), hour = ((in_hr+1):(out_hr+23))%%24)
    }, in_hr = login_hour, out_hr = logout_hour, SIMPLIFY = F)
  ))
  # what's left?
  mid <- logout_hour > (login_hour + 1)
  inday <- rbindlist(with(SD[mid, list(login_hour, logout_hour)],
    mapply(function(in_hr, out_hr) {
      data.table(usage = rep.int(3600, out_hr-in_hr-1), hour = (in_hr+1):(out_hr-1))
    }, in_hr = login_hour, out_hr = logout_hour, SIMPLIFY = F)
  ))
  usage_by_hour <- with_usage_zeros(rbind(login_usage_by_hour, logout_usage_by_hour, cross_day, inday)[, list(usage = sum(usage)/total_login_time), keyby=hour], hourly_zeros)
  
  hourly_percent <- with(usage_by_hour, {
    names(usage) <- paste0("hr", hour)
    usage
  })

  return(c(list(
      log10_lifetime = log10(life),
      log10_total_duration = log10(total_login_time),
      log10_unique_users = log10(user_count),
      ave_duration = mean(duration), sd_duration = sd(duration)
    ),
    weekday_percent, monthly_percent, hourly_percent,
    as.list(dur_quantiles), as.list(login_hours), as.list(logout_hours)
  ))
})

digestor <- function(dt, preds, training, cache = "../input/initPred.RData") {
  { if (!file.exists(cache)) {
    d <- dt[, preds(.SD), by=location_id]
    saveRDS(d, cache)
    d
  } else {
    readRDS(cache)
  } } -> ref
  
  centered <- ref[,lapply(.SD[,-1,with=F],function(col) col-mean(col))]
  scaled <- cbind(location_id = ref$location_id, centered[,lapply(.SD, function(col) col/sd(col))])
  list(raw = ref, training = scaled[training], validation = scaled[!training])
}
redigest <- function(ref, training) {
  centered <- ref[,lapply(.SD[,-1,with=F],function(col) col-mean(col))]
  scaled <- cbind(location_id = ref$location_id, centered[,lapply(.SD, function(col) col/sd(col))])
  list(raw = ref, training = scaled[training], validation = scaled[!training])
}
dopca <- function(digest) with(digest, {
  pca <- prcomp(training[,-1,with=F], center=F, scale. = F)
  val <- predict(pca, validation[,-1,with=F])
  return(list(pca=pca, val=val))
})
initial_digest <- digestor(censor.dt, refpredictors, training_locations)
```

Let's have a look at the time signature information first:

```{r, echo=F, include=F}
temporal_data_slice <- melt(initial_digest$raw[, .SD, .SDcols = c(1, grep("hr", names(initial_digest$raw)))], id.var = "location_id")
temporal_data_slice[grepl("log", variable), measure := sub("_hr_.+","", variable)]
temporal_data_slice[!grepl("log", variable), measure := "usage"]
temporal_data_slice[, hour:= as.integer(gsub("[^\\d]","", variable, perl=T))]
setkey(temporal_data_slice, location_id, measure, hour)
```

```{r, echo=F, fig.cap="Raw proportions by hour"}
p <- ggplot(temporal_data_slice) + theme_bw() + aes(group = location_id, x = hour, y = value) + facet_grid(measure ~ .) + geom_line(alpha=0.2) + scale_x_discrete(breaks=0:23, expand=c(0,0)) +
  scale_y_continuous("proportion", expand=c(0,0))
ggsave("time-sig.png", p, width=7, height=4)
p
```

```{r, echo=F, fig.cap="Measure Peak distribution"}
peaks <- temporal_data_slice[, list(peak=which.max(value)-1), by=list(location_id, measure)]
p <- ggplot(peaks) + theme_bw() + aes(x = peak) + facet_grid(measure ~ .) +
  geom_bar() +
  scale_x_discrete("peak hour", limits=0:23)
p
```

```{r, echo=F, fig.cap="Frequency Power Signatures"}
freq_power <- function(ref, measure) {
  mn <- mean(as.matrix(ref[,-1,with=F]))
  fftref <- mvfft(t(ref[,-1,with=F])-mn)
  pwr <- Re(fftref*Conj(fftref))[1+1:12,]
  res <- t(pwr)
  dimnames(res)[[2]] <- paste0(as.integer(gsub("[^\\d]","",dimnames(res)[[2]], perl=T)), "_perday")
  cbind(melt(data.table(cbind(location_id = ref[,location_id], res)), id.var = "location_id"), measure = measure)
}

ref_usage <- initial_digest$raw[,.SD,.SDcols=c(1,grep("^hr", names(initial_digest$raw)))]
ref_login <- initial_digest$raw[,.SD,.SDcols=c(1,grep("^login", names(initial_digest$raw)))]
ref_logout <- initial_digest$raw[,.SD,.SDcols=c(1,grep("^logout", names(initial_digest$raw)))]
pwr <- rbind(
  freq_power(ref_usage, "usage"),
  freq_power(ref_login, "login"),
  freq_power(ref_logout, "logout")
)

p <- ggplot(pwr[,
  list(power=value, frequency=as.numeric(sub("_perday","", variable))), by=list(location_id, measure)
]) + theme_bw() + aes(y = power, x=frequency, group=location_id) + facet_grid(measure ~ .) + geom_line(alpha=0.2) + scale_x_continuous("per day frequency", breaks=c(1,2,3,4,6,8,12))
# pwr[, list(peak_freq = variable[which.max(value)]), by=list(location_id, measure)][,.N,by=list(measure, peak_freq)]
p
```

By-eye conclusions:

 - peak login time does not separate into clean groups.  So, separate into early, middle, and late thirds.
 - peak logouts and usage, however, seem to identify three lumps, so we will use mixture von Mises models to identify clusters.
 - some locations have substantially different power (less than .1 vs greater than 1) in the frequencies below the Nyquist limit (fixed 24 samples per day, so limited to detecting frequences 12 per day)
 - most locations have peak power on a daily cycle, but a few do not
 
Categorizing on these features:

```{r,echo=F,include=F}
# http://stackoverflow.com/questions/18788748/fit-a-mixture-of-von-mises-distributions-in-r
psi <- peaks[measure == "usage", peak/24*pi] # usage angle
tht <- peaks[measure == "login", peak/24*pi] # login angle
phi <- peaks[measure == "logout", peak/24*pi] # logout angle
x0 <- cos(psi)
x1 <- sin(psi)*cos(tht)
x2 <- sin(psi)*sin(tht)*cos(phi)
x3 <- sin(psi)*sin(tht)*sin(phi)
pos <- cbind(x0,x1,x2,x3)

#lls <- lapply(8:11, function(k) movMF(pos, k, list(maxiter=10000, nruns=20)))
#bics <- lapply(lls, stats::BIC)
vmfmodel <- movMF(pos, 3, list(maxiter=10000, nruns=20))
temporal_cluster <- data.table(location_id = peaks[measure == "usage"]$location_id, vMFcluster = predict(vmfmodel, pos), key="location_id")
vmfrename <- peaks[measure == "usage"][temporal_cluster][,
  list(med=median(peak)),by=vMFcluster
][,vMFcluster, keyby=med][,list(vMFcluster, rename=factor(c("early","middle","late"), levels=c("early","middle","late"), ordered = T))]
temporal_cluster <- merge(temporal_cluster, vmfrename, by="vMFcluster")[,list(vMFcluster=rename), keyby=location_id]

#alt_temporal_cluster <- data.table(location_id = peaks[measure == "usage"]$location_id, vMFcluster = predict(lls[[1]], pos), key="location_id")

# usage_peak_hr <- peaks[measure == "usage", peak]
# ang <- usage_peak_hr/12*pi
# unit_circ <- cbind(cos(ang), sin(ang))
# predusage <- predict(movMF(unit_circ, 3, list(maxiter=10000, nruns=20)), unit_circ)
# peaks[measure == "usage", cluster := paste0("usage_", predusage[location_id])]
# 
# logout_peak_hr <- peaks[measure == "logout", peak]
# ang <- logout_peak_hr/12*pi
# unit_circ <- cbind(cos(ang), sin(ang))
# predlogout <- predict(movMF(unit_circ, 3, list(maxiter=10000, nruns=20)), unit_circ)
# peaks[measure == "logout", cluster := paste0("logout_", predlogout[location_id])]
# 
# login_peak_hr <- peaks[measure == "login", peak]
# splits <- quantile(login_peak_hr, probs = c(1,2)/3)
# login_clusters <- ifelse(login_peak_hr <= splits[1], "low",ifelse(login_peak_hr <= splits[2],"mid", "high"))
# peaks[measure == "login", cluster := login_clusters[location_id]]
```

```{r, echo=F, fig.cap="Login, Logout, Usage Peak Clusters"}
p <- ggplot(peaks[temporal_cluster]) + theme_bw() + aes(x = peak, fill=vMFcluster) + facet_grid(measure ~ .) + geom_bar(binwidth=1) + labs(fill="von Mises-Fisher\n3-Sphere\nCluster\nPeak Time")
ggsave("time-peaks.png", p, width=7, height=4)
p
```

Let's turn to the powers:

```{r, echo=F, fig.cap="Frequency Power Clusters"}
setkey(pwr, measure, location_id, variable)
peak_freq_cat <- pwr[,list(peak_freq = ifelse(variable[which.max(value)] == "1_perday","daily","multimode")), keyby=list(measure, location_id)]
tot_pwr <- pwr[,list(tot_pwr = sum(value)), keyby=list(measure, location_id)]
pwr_divisions <- tot_pwr[,{
  res <- as.list(quantile(tot_pwr, probs = c(1,2)/3))
  names(res) <- c("low","hi")
  res
}, keyby=measure]
pwr_cluster <- tot_pwr[pwr_divisions][peak_freq_cat][,
  list(peak_freq, pwr_cat = ifelse(tot_pwr < low,"lo",ifelse(tot_pwr < hi, "med","hi"))),
  keyby=list(measure, location_id)
]
p <- ggplot(pwr[pwr_cluster][,
  list(power=value, frequency=as.numeric(sub("_perday","", variable)), peak_freq, pwr_cat),
  by=list(location_id, measure)
]) + theme_bw() + aes(y = power, x=frequency, group=location_id, alpha=peak_freq, color = pwr_cat) + facet_grid(measure ~ .) + geom_line() + scale_x_continuous("per day frequency", breaks=c(1,2,3,4,6,8,12))
ggsave("freq-powers.png", p, width=7, height=4)
p
```

Showing the power cluster breakdown
```{r}
pre_pwr_clust <- data.table(dcast(pwr_cluster, location_id ~ measure, value.var = "pwr_cat"))
pwr_multi_clusters <- setorder(pre_pwr_clust[,.N,by=list(login, logout, usage)], -N)
pwr_multi_clusters
reduced_pwr_clust <- pre_pwr_clust[, list(pwr_clust = factor(ifelse((login == logout) || (login == usage), login, logout), levels=c("lo","med","hi"), ordered=T)), keyby=location_id]
```
Probably worth having all-(low|med|high) clusters; the other cases all (with one exception) are two the same, the third off by one.


And now the peak frequency clusters:
```{r}
freq_multi_clusters <- setorder(data.table(dcast(pwr_cluster, location_id ~ measure, value.var = "peak_freq"))[,.N,by=list(login, logout, usage)], -N)
freq_multi_clusters
```
All daily is the vast majority.  Not worth attempting to categorize by this dimension, except *possibly* as all daily vs. not.  Then again, maybe makes sense to remove daily signal, and then look at what remains?

note: 1 yr ~= log10(lifetime) == 7.5
 
Finally, we can look at the relative usage per user:
\newpage\blandscape
```{r, echo=F, fig.width=11.25, fig.height=7, fig.cap="Locations clustered excess usage per user."}
agg.dt <- initial_digest$raw[,list(log10_total_duration,log10_unique_users,log10_lifetime), keyby=location_id]
lg10lm <- lm(log10_total_duration-log10_unique_users ~ log10_lifetime, agg.dt)
agg.dt <- cbind(agg.dt, predict(lg10lm, agg.dt, interval="prediction", level=1/3))[,
  lifetime_cat := factor(ifelse(log10_total_duration-log10_unique_users < lwr, "low",
                  ifelse(log10_total_duration-log10_unique_users < upr, "mid", "high")), levels=c("low","mid","high"), ordered=T)
][reduced_pwr_clust][temporal_cluster]
p <- ggplot(agg.dt) + theme_bw() +
  aes(y = log10_total_duration-log10_unique_users, x = log10_lifetime, color = vMFcluster, size = pwr_clust, alpha = pwr_clust) +
  with(agg.dt, annotate("ribbon", x=log10_lifetime, ymax=upr, ymin=lwr, fill="lightgrey")) +
  geom_vline(xintercept=log10(365*24*60*60), color="grey") +
  annotate("text", x=log10(365*24*60*60)+c(-0.02,0.02), y=5.75, label=c("< 1 year","> 1 year"), angle=90, color="grey") +
  geom_point() +
#  geom_ribbon(aes(ymax=upr, ymin=lwr, y=NULL, color=NULL, size=NULL), alpha=0.2) +
  labs(
    x = expression(log[10]("lifetime")),
    y = expression(log[10]*bgroup("(",frac("total usage","# unique users"),")")),
    color = "von Mises-Fisher\nPeak Time\nCluster"
  ) + scale_size_manual("power\ncluster", limits=c("low","med","hi"), values=c(2,5,8)) +
  scale_alpha_manual("power\ncluster", limits=c("low","med","hi"), values=(2^0.5)/c(2,5,8)^0.5)
ggsave("locationclusters.png", p, width=7, height=4)
p
```
\elandscape\newpage

\newpage\blandscape
```{r, echo=F, fig.width=11.25, fig.height=7, fig.cap="Raw proportions by hour"}
setkey(pwr_cluster, location_id, measure)
p <- ggplot(temporal_data_slice[temporal_cluster][pwr_cluster]) + theme_bw() +
  aes(group = location_id, x = hour, y = value, color=pwr_cat) +
  facet_grid(measure ~ vMFcluster) + geom_line(alpha=0.3) +
  scale_x_discrete(breaks=0:23, expand=c(0,0)) +
  scale_y_continuous("proportion", expand=c(0,0))
p
```
\elandscape\newpage

\newpage\blandscape
```{r, echo=F, fig.width=11.25, fig.height=7, fig.cap="random sample of usage paths"}
## select some users w/ 5+ locations
travelers <- sample(censor.dt[,list(ul=length(unique(location_id))),by=user_id][ul >= 5, user_id], 5)
trips <- setkey(setnames(
  censor.dt[user_id %in% travelers, location_id, by=user_id][,
    rle(location_id), by=user_id
  ], "values","location_id"), user_id)
locations <- agg.dt[,
  list(x=log10_lifetime, y=log10_total_duration-log10_unique_users),
  by=location_id
]
paths <- trips[,cbind(locations[location_id, list(x, y)], lengths, agg.dt[location_id, list(lifetime_cat, pwr_clust, vMFcluster)], user=.GRP), by=user_id]
p + geom_path(aes(x=x, y=y, linetype=factor(user), color=NULL, alpha=NULL, size=NULL), paths) +
  with(paths[lengths > 1],
    annotate("point", x=x, y=y, shape=c(0,1,2,5,6)[user], size=10*sqrt(lengths/max(lengths)))
  ) +
  guides(color="none", alpha="none", size="none") + labs(linetype="user")
```
\elandscape\newpage

location diversity - share of locations in each type, in each dimension.  We know by construction that location diversity is uniform for low, medium, and high usage per lifetime categorization: we created the categories based on quantiles.  Same is true for use variation frequency power.  For the clusters of peak utilization characteristics, however, 

```{r, echo=F}
multinomtest <- function(xs, nh=rep(1/length(xs), times=length(xs))) {
  N <- sum(xs)
  2*N*log(N) - 2*sum(xs*log(nh/xs))
}
agg.dt[,.N,by=lifetime_cat][,list(lifetime_cat, share=N/sum(N))]
agg.dt[,.N,by=pwr_clust][,list(pwr_clust, share=N/sum(N))]
agg.dt[,.N,by=vMFcluster][,list(vMFcluster, share=N/sum(N))]
```


Now if take these clusters and characterize user visits in terms of cluster visits, instead of particular location visits, we can consider their usage of cluster "types".  There are two main questions: how much (relatively) do individuals use a type? and how often do they rotate between types?

```{r, echo=F}
cluster_rle_src <- merge(censor.dt, agg.dt, by="location_id")
setkey(cluster_rle_src, login)
ref <- censor.dt[,
  list(
    ul=length(unique(location_id)),
    life=max(logout)-min(login),
    usage=sum(logout-login)),
  keyby=user_id
]

categorizer <- function(src.dt, cat, ref) {
  rle_expr <- paste0("rle(",cat,")")
  key_expr <- c("user_id",cat)
  lvls <- levels(src.dt[[cat]])
  mx_expr <- parse(text=paste0("list(main=factor(",cat,"[which.max(prop)], ordered=T, levels=lvls),share=max(prop))"))
  rle_dt <- src.dt[, rle(as.numeric(eval(parse(text=cat)))), keyby=user_id]
  prop_ref <- src.dt[,list(prop=sum(logout-login)), keyby=key_expr][ref][,list(prop=prop/usage), keyby=key_expr]
  prop_lab <- prop_ref[, eval(mx_expr), by=user_id]
  chunks <- data.table(dcast(rle_dt, user_id ~ values, value.var="lengths", fun.aggregate = sum), key="user_id")
  ## cast data, maybe?
  digest <- rle_dt[, list(tot_visits = sum(lengths), mn_reps=mean(lengths), switches=.N), keyby=user_id][ref][prop_lab][chunks]
  list(rle_dt=rle_dt, prop_ref=prop_ref, prop_lab=prop_lab, digest=digest)
}

lifetime_dts <- categorizer(cluster_rle_src, "lifetime_cat", ref)
pwr_dts <- categorizer(cluster_rle_src, "pwr_clust", ref)
vMF_dts <- categorizer(cluster_rle_src, "vMFcluster", ref)

# path_by_cluster <- paths[, list(user_id, lengths, loc_cluster = interaction(lifetime_cat, pwr_clust, vMFcluster))]
```




\newpage\blandscape
```{r, echo=F, fig.width=11.25, fig.height=7, fig.cap="distribution of main location type usage"}
digests <- rbind(
  lifetime_dts$digest[,list(switch_rate = switches/usage, main, share, type="usage")],
  pwr_dts$digest[,list(switch_rate = switches/usage, main, share, type="power")],
  vMF_dts$digest[,list(switch_rate = switches/usage, main, share, type="peaktime")]
)
p <- ggplot(digests) + facet_grid(type ~ .) + theme_bw() + aes(x=log10(switch_rate), fill=main, alpha=factor(floor(share*10))) + geom_bar(binwidth=0.1) + scale_alpha_manual(breaks=0:10, values=seq(0.5,1,length.out = 11))
p
```
\elandscape\newpage

\newpage\blandscape
```{r, echo=F, fig.width=11.25, fig.height=7, fig.cap="normalized distribution of main location type usage"}
p <- ggplot(digests) + facet_grid(type ~ .) + theme_bw() + aes(x=log10(switch_rate), fill=main, alpha=factor(floor(share*10))) + geom_bar(binwidth=0.1, position="fill") + scale_alpha_manual(breaks=0:10, values=seq(0.5,1,length.out = 11))
p
```
\elandscape\newpage

\newpage\blandscape
```{r, echo=F, fig.width=11.25, fig.height=7, fig.cap="user categorization, by primary location type utilization"}
plot_ref <- ref[
  lifetime_dts$digest[, list(lifetime_main = main, lifetime_share = share), keyby=user_id]
][
  pwr_dts$digest[, list(pwr_main = main, pwr_share = share), keyby=user_id]
][
  vMF_dts$digest[, list(peak_main = main, peak_share = share), keyby=user_id]
]
user_src.dt <- plot_ref[between(life, 365*24*60*60, 3*365*24*60*60) & between(ul,3,20)]
saveRDS(user_src.dt, "../input/user.RData")
#thing[, list(user_id, mains=interaction(lifetime_main, pwr_main, peak_main))][,.N,by=mains]
p <- ggplot(user_src.dt[, list(y=log10(usage), x=log10(life), pwr_main, lifetime_main, peak_main, membership = lifetime_share * pwr_share * peak_share)]) +facet_grid(pwr_main ~ lifetime_main, labeller = function(vardt){
  if (!is.null(vardt$pwr_main)) {
    list(c(lo="smooth usage",med="some usage variability",hi="highly variable usage")[as.character(vardt$pwr_main)])
  } else {
    list(c(low="low usage per time", mid="typical usage per time",high="high usage per time")[as.character(vardt[[1]])])
  }
}) + aes(x=x,y=y, color=factor(peak_main), alpha = membership) + geom_point() + theme_bw() + labs(y="log(usage)", x="log(life)", color="usage\npeak")
ggsave("users.png", p)
p
```
\elandscape\newpage

```{r, echo=F}
saveRDS(agg.dt[,list(lifetime_cat, pwr_clust, vMFcluster), by=location_id], "../input/locClusters.RData")
intermediate <- censor.dt[user_id %in% user_src.dt$user_id, list(visits = .N), by=list(user_id, location_id)][,
  list(location_id, pref=visits/sum(visits)), keyby=user_id
][,list(user_id, pref), keyby=location_id]
saveRDS(intermediate[agg.dt[location_id %in% unique(intermediate$location_id), list(lifetime_cat, pwr_clust, vMFcluster), keyby=location_id]][,
  list(lifetime_cat, pwr_clust, vMFcluster, pref),
  keyby=user_id
],"../input/userPrefs.RData")

#[,.N,keyby=list(user_id, lifetime_cat, pwr_clust, vMFcluster)][,list(pref = N/sum(N),keyby=list(user_id, lifetime_cat, pwr_clust, vMFcluster)]
# thing <- 
#   data.table(
#     x=lifetime_dts$digest[, list(pos = (0.5*`2`+1*`3`)/(`1`+`2`+`3`), main, share, user_id)]$pos,
#     y=pwr_dts$digest[, list(pos = (0.5*`2`+1*`3`)/(`1`+`2`+`3`), main, share, user_id)]$pos,
#     z=vMF_dts$digest[, list(pos = (0.5*`2`+1*`3`)/(`1`+`2`+`3`), main, share, user_id)]$pos,
#     key=c("x","y","z")
#   )
# uthing <- unique(thing)
# lenthing <- dim(uthing)[1]
# cloud(z ~ x*y, data=unique(thing)[sample.int(lenthing, 250)])
# censor.dt[user_id %in% user_src.dt$user_id, .N*60*60*24*30/(max(logout)-min(login)), by=user_id][V1>1]
```


```{r}
slice <- user_src.dt[lifetime_main=="mid" & pwr_main == "med" & peak_main == "middle"]
mlt.slice <- melt.data.table(slice[,list(life,ulog=log10(usage), unilocs=ul),by=user_id], id.vars = "user_id", measure.vars = c("life","ulog","unilocs"))
ggsave("mmm_life.png", ggplot(mlt.slice[variable == "life"]) + theme_bw() + aes(x=value, y=..density..) + geom_histogram() + labs(x="lifetime (end - start)", y="density"), width=7, height=4)
ggsave("mmm_usage.png", ggplot(mlt.slice[variable == "ulog"]) + theme_bw() + aes(x=value, y=..density..) + geom_histogram() + labs(x="log10(usage)", y="density"), width=7, height=4)
ggsave("mmm_ulocs.png", ggplot(mlt.slice[variable == "unilocs"]) + theme_bw() + aes(x=value, y=..density..) + geom_histogram() + labs(x="unique locations", y="density"), width=7, height=4)

slice <- user_src.dt[lifetime_main=="mid" & pwr_main == "lo" & peak_main == "early"]
mlt.slice <- melt.data.table(slice[,list(life,ulog=log10(usage), unilocs=ul),by=user_id], id.vars = "user_id", measure.vars = c("life","ulog","unilocs"))
ggsave("mle_life.png", ggplot(mlt.slice[variable == "life"]) + theme_bw() + aes(x=value, y=..density..) + geom_histogram() + labs(x="lifetime (end - start)", y="density"), width=7, height=4)
ggsave("mle_usage.png", ggplot(mlt.slice[variable == "ulog"]) + theme_bw() + aes(x=value, y=..density..) + geom_histogram() + labs(x="log10(usage)", y="density"), width=7, height=4)
ggsave("mle_ulocs.png", ggplot(mlt.slice[variable == "unilocs"]) + theme_bw() + aes(x=value, y=..density..) + geom_histogram() + labs(x="unique locations", y="density"), width=7, height=4)
```

```{r}
mmm.cc <- loclifetimes[setkey(fread("../../muri-overall/output/matched/mid/med/middle/10/001-covert-0-cc.csv", col.names = c("user.a","user.b","location_id","start","end","other")), location_id)][start < i.end & i.start < end, list(start=i.start, end=i.end, user.a, user.b, location_id)]
mmm.cu <- loclifetimes[setkey(fread("../../muri-overall/output/matched/mid/med/middle/10/001-covert-0-cu.csv", col.names = c("user.a","user.b","location_id","start","end","other")), location_id)][start < i.end & i.start < end, list(start=i.start, end=i.end, user.a, user.b, location_id)]
mmm.all <- rbind(mmm.cc, mmm.cu)
mmm.both <- rbind(
  mmm.all[user.a < 0, list(mx=max(end),mn=min(start),sm=sum(end-start),ul=length(unique(location_id))), by=list(user_id=user.a)],
  mmm.all[user.b < 0, list(mx=max(end),mn=min(start),sm=sum(end-start),ul=length(unique(location_id))), by=list(user_id=user.b)]
)[,list(life=max(mx)-min(mn), usage=sum(sm), ul=sum(ul)),by=user_id]
```

```{r}
mle.cc <- loclifetimes[setkey(fread("../../muri-overall/output/matched/mid/lo/late/10/001-covert-0-cc.csv", col.names = c("user.a","user.b","location_id","start","end","other")), location_id)][start < i.end & i.start < end, list(start=i.start, end=i.end, user.a, user.b, location_id)]
mle.cu <- loclifetimes[setkey(fread("../../muri-overall/output/matched/mid/lo/late/10/001-covert-0-cu.csv", col.names = c("user.a","user.b","location_id","start","end","other")), location_id)][start < i.end & i.start < end, list(start=i.start, end=i.end, user.a, user.b, location_id)]
mle.all <- rbind(mle.cc, mle.cu)
mle.both <- rbind(
  mle.all[user.a < 0, list(mx=max(end),mn=min(start),sm=sum(end-start),ul=length(unique(location_id))), by=list(user_id=user.a)],
  mle.all[user.b < 0, list(mx=max(end),mn=min(start),sm=sum(end-start),ul=length(unique(location_id))), by=list(user_id=user.b)]
)[,list(life=max(mx)-min(mn), usage=sum(sm), ul=sum(ul)),by=user_id]
```

show swtiching rate normalized by visit rate on a per user basis

show 27 categories

group sizes 5-20 by 5, also a 30
meeting frequency - once a week, once every two weeks, once every month
try
 - meeting at less probable times
 - meeting location be inconsistent with normal behavior

detection based on cycles