---
output: pdf_document
---
## Analysis and plots of montreal data.

```{r, echo=FALSE, include=FALSE}
req.packages <- c("data.table", "ggplot2", "reshape2", "grid")
sapply(req.packages, require, character.only = T)
```

### Required packages: `r req.packages`.

Source data:

```{r, echo=FALSE}
src.dt <- 
  setkey(
    setnames(
      fread("../input/merged.o", header = F, sep=" ", colClasses = list(integer=1.4)),
      c("user_id", "location_id", "login", "logout")
    ),
    login, logout, user_id, location_id
  )
src.dt
```

```{r, echo=FALSE}
max_hours <- 24
min_logins <- 2
```

Trim data:

1. remove sessions exceeding `r max_hours` hours
2. remove sessions with no time (these correspond to rows in the source data with no logout time)
3. eliminate users that have fewer than `r min_logins` logins
4. eliminate locations that have fewer than `r min_logins` logins
5. recursively apply steps 2-3 until no users / locations eliminated

```{r, echo=FALSE}
censor.dt <- src.dt[(logout != login) & ((logout - login) <= max_hours*60*60), ]
invalid.users <- censor.dt[,.N, by=user_id ][N < min_logins, user_id]
while(length(invalid.users) > 0) {
  censor.dt     <- censor.dt[!user_id %in% invalid.users]
  invalid.locs  <- censor.dt[,.N, by=location_id ][N < min_logins, location_id]
  censor.dt     <- censor.dt[!location_id %in% invalid.locs]
  invalid.users <- censor.dt[,.N, by=user_id ][N < min_logins, user_id]
}
censor.dt
```

Removed `r dim(src.dt)[1] - dim(censor.dt)[1]` rows, or `r (dim(src.dt)[1] - dim(censor.dt)[1])/dim(src.dt)[1]` reduction.

Relabel source data:

1. re-assign user ids from 1, ordered by first appearance
2. re-assign location ids from 1, ordered by first appearance
3. TODO split entries that cross days (~ 3% of data; of crossing entries, most look like late PM->early AM sessions, aka bar / club visits)
4. convert login / logout times to `log(in|out)_day` and `log(in|out)_time`
5. mark data as the empirical source: set `run_id = 0`, `sample_id = 0`, `target = FALSE`.

```{r, echo=FALSE}
source("munging.R")
setkey(breakoutDays(censor.dt[,
  user_id := .GRP, by=user_id
][,
  location_id := .GRP, by=location_id
]), login_day, login_time, logout_day, logout_time, user_id, location_id)
censor.dt
```

Now we must try to understand the remaining data.

First, what happens with our hotspot locations?  We need to understand how people appear connected to them, in order to properly model how our covert agents might use them.  To do that, we need to speculate about how these hotspots are behaving, *e.g.* are they new wifi access locations? are they substituting for existing services?

We can start by looking at all of locations, and their first and last access times.

```{r, echo=FALSE}
minmax.dt <- censor.dt[,list(t_min=min(login),t_max=max(logout)), keyby=location_id]
ggplot(minmax.dt) +
  theme_bw() +
  aes(x=location_id, ymin=t_min/60/60/24/7, ymax=t_max/60/60/24/7) +
  geom_linerange() +
  coord_flip() + labs(y="weeks since start", x="hotspot")
```

The fairly consistent slope of site start times indicates a reliable creation rate.  We can investigate this further by considering the average creation rate for the whole time period, and the rolling average.

We can determine the overall average creation rate straightforwardly, by taking the number of hotspots created over the whole duration of the data set.  We should also compare that average to an aggregation (*e.g.*, the number created in each week) to get a feel for how creation varies around that.

```{r, echo=FALSE}
weekly_ave <- minmax.dt[,
  (length(unique(location_id))-2)/(max(t_max) - min(t_min))*60*60*24*7
]
weekly.dt <- minmax.dt[,
  list(
    creation = (t_min/60/60/24)%/%7,
    shutdown = (t_max/60/60/24)%/%7
  ),
  by = location_id
]
count.dt <- weekly.dt[,
  .N, by = creation
]
ggplot(count.dt) + theme_bw() + 
  aes(x=creation, y=N) +
  labs(x="week", y="hotspots created", color="ave") +
  geom_bar(stat="identity", width=1) +
  geom_hline(y = weekly_ave, color="red")
```

We can also be a bit more statistical, fitting a least squares regression model (here, creations by day instead of by week):

```{r, echo=FALSE}
creations <- setkey(censor.dt[,
  list(creation_day=min(login_day)),
  by=location_id
][,
  .N,
  by=creation_day
][,
  list(creation_day = creation_day - min(creation_day), N)
], creation_day)
zeros <- data.table(
  creation_day = 0:max(creations$creation_day),
  N = 0,
  key = "creation_day"
)
creations <- merge(creations, zeros, all=TRUE)[,
  list(N = max(N.x, N.y, na.rm=T)),
  keyby = creation_day
]
ggplot(creations) + theme_bw() +
  aes(x=creation_day, y=N) +
  stat_smooth(method="lm") + geom_point() +
  labs(x="creation day", y="hotspots created")
```

This leaves a bit to be desired - clearly, there is a mostly constant level with possibly a very small trend, but least squares is not the best fitting approach since the underlying data is restricted to be positive.  We can use maximum likelihood to better describe the number of new hotspots created per day.  We can posit some processes, like creation of new businesses, tendency to accummulate requests and complete them all on one day, *etc.* to inspire our distribution.

TODO: zero-inflated poisson?  zero-inflated negative binomial?

Unlike creation, elimination of hotspot IDs seems to be much more variable.  While they are steadily created, their lifetime after creation has a long tailed distribution.  This is further complicated by the termination of the data, left censoring many entries.

TODO: break out entries that start near the end of the dataset and highlight their contribution to the spike in low weeks.

```{r,echo=FALSE}
dt <- minmax.dt[,
  list(duration = (t_max-t_min)/60/60/24/7),
  by=location_id
]
# h <- hist(dt$duration, 0:ceiling(max(dt$duration)), plot=F)
ggplot(dt) +
  theme_bw() +
  aes(x=duration) +
  geom_bar(binwidth=1) + labs(x="weeks of hotspot life", y="hotspots with that duration")
```

The usage profile of hostspots also varies.  We can use this to categorize "types" of locations using a *k-means* approach.  First, let's look at grouping by typical *login* and *logout* times:

```{r, echo=FALSE}
shft <- 4
orig <- 0:23
hr_shf <- (orig - shft) %% 24
hr_lab <- (orig + shft) %% 24
base <- censor.dt[,
  list(
    login_hour  = floor(login_time/60/60),
    logout_hour = floor(logout_time/60/60)
  ),
  by = location_id
]
both <- rbind(base[,
  list(login_count=.N),
  keyby=list(location_id, login_hour)
][,
  list(
    hour=hr_shf[login_hour+1],
    count=login_count/max(login_count),
    event=factor("login", levels=c("logout","login"))
  ),
  keyby=location_id
],
base[,
  list(logout_count=.N),
  keyby=list(location_id, logout_hour)
][,
  list(
    hour=hr_shf[logout_hour+1],
    count=logout_count/max(logout_count),
    event=factor("logout", levels=c("logout","login"))
  ),
  keyby=location_id
])
setkey(both, location_id, event, hour)
# by max vs by sum?
byid <- acast(both, location_id ~ hour + event, value.var = "count", fill = 0)

kmplotter <- function(n, castdata, src,
  facet = facet_grid(event ~ cluster, scales = "free"),
  refevent = "login"
) {
    km <- kmeans(castdata, n, nstart = 5)
    cluster.dt <- data.table(
      location_id = 1:src[,max(location_id)],
      cluster = km$cluster, 
      key="location_id"
    )
    peakhours <- src[cluster.dt][,
      list(peak=sum(hour*count)/sum(count)),
      by=list(event, cluster)
    ]
    setkey(peakhours, event, peak)
    ## get the order from login
    ## relabel
    reorder <- peakhours[event == refevent, order(cluster)]
    peakhours[, cluster := reorder[cluster]]
    cluster.dt[,cluster := reorder[cluster]]
  # reorder cluster ids
    ggplot(src[cluster.dt]) +
      theme_bw() +
      aes(x = factor(location_id), y = hour, fill = count) + facet +
      geom_raster() + geom_hline(data=peakhours, mapping=aes(yintercept = peak), color="red") +
      scale_x_discrete("location",  labels="") +
      scale_y_discrete(name="hour", labels=hr_lab, limits=0:23, expand = c(0,0)) +
      labs(fill="prop. max count")
}
# print(kmplotter(3))
# print(kmplotter(4))
print(kmplotter(5, byid, both)) 
# print(kmplotter(6))
# print(kmplotter(7))
# print(kmplotter(8))
# print(kmplotter(9))
# print(kmplotter(10))
# TODO: sort out clustering - either different clustering algorithm, or principled selection of k?
```

An alternative view would be to consider something like total users present in each hour.

TODO: could weight a user contribution to an hour by how much time they are present during it, but seems reasonable to just do counts at the moment.

```{r, echo=FALSE}
if (file.access("extend.RData") != 0) {
  proto <- array(0L, dim = c(1,24), dimnames = list(drop=c(),hours=0:23))
  extend <- cbind(censor.dt, proto)
  extend[, login_hour := floor(login_time/60/60) ][, logout_hour:=floor(logout_time/60/60)]
  extend[, "0"  := ifelse((0  <= logout_hour) & (login_hour <= 0),  1L, 0L)]
  extend[, "1"  := ifelse((1  <= logout_hour) & (login_hour <= 1),  1L, 0L)]
  extend[, "2"  := ifelse((2  <= logout_hour) & (login_hour <= 2),  1L, 0L)]
  extend[, "3"  := ifelse((3  <= logout_hour) & (login_hour <= 3),  1L, 0L)]
  extend[, "4"  := ifelse((4  <= logout_hour) & (login_hour <= 4),  1L, 0L)]
  extend[, "5"  := ifelse((5  <= logout_hour) & (login_hour <= 5),  1L, 0L)]
  extend[, "6"  := ifelse((6  <= logout_hour) & (login_hour <= 6),  1L, 0L)]
  extend[, "7"  := ifelse((7  <= logout_hour) & (login_hour <= 7),  1L, 0L)]
  extend[, "8"  := ifelse((8  <= logout_hour) & (login_hour <= 8),  1L, 0L)]
  extend[, "9"  := ifelse((9  <= logout_hour) & (login_hour <= 9),  1L, 0L)]
  extend[, "10" := ifelse((10 <= logout_hour) & (login_hour <= 10), 1L, 0L)]
  extend[, "11" := ifelse((11 <= logout_hour) & (login_hour <= 11), 1L, 0L)]
  extend[, "12" := ifelse((12 <= logout_hour) & (login_hour <= 12), 1L, 0L)]
  extend[, "13" := ifelse((13 <= logout_hour) & (login_hour <= 13), 1L, 0L)]
  extend[, "14" := ifelse((14 <= logout_hour) & (login_hour <= 14), 1L, 0L)]
  extend[, "15" := ifelse((15 <= logout_hour) & (login_hour <= 15), 1L, 0L)]
  extend[, "16" := ifelse((16 <= logout_hour) & (login_hour <= 16), 1L, 0L)]
  extend[, "17" := ifelse((17 <= logout_hour) & (login_hour <= 17), 1L, 0L)]
  extend[, "18" := ifelse((18 <= logout_hour) & (login_hour <= 18), 1L, 0L)]
  extend[, "19" := ifelse((19 <= logout_hour) & (login_hour <= 19), 1L, 0L)]
  extend[, "20" := ifelse((20 <= logout_hour) & (login_hour <= 20), 1L, 0L)]
  extend[, "21" := ifelse((21 <= logout_hour) & (login_hour <= 21), 1L, 0L)]
  extend[, "22" := ifelse((22 <= logout_hour) & (login_hour <= 22), 1L, 0L)]
  extend[, "23" := ifelse((23 <= logout_hour) & (login_hour <= 23), 1L, 0L)]
} else {
  load("extend.RData")
}
if (file.access("comp.RData") != 0) {
  comp <- extend[,list(
    `0` =sum(`0`),
    `1` =sum(`1`),
    `2` =sum(`2`),
    `3` =sum(`3`),
    `4` =sum(`4`),
    `5` =sum(`5`),
    `6` =sum(`6`),
    `7` =sum(`7`),
    `8` =sum(`8`),
    `9` =sum(`9`),
    `10`=sum(`10`),
    `11`=sum(`11`),
    `12`=sum(`12`),
    `13`=sum(`13`),
    `14`=sum(`14`),
    `15`=sum(`15`),
    `16`=sum(`16`),
    `17`=sum(`17`),
    `18`=sum(`18`),
    `19`=sum(`19`),
    `20`=sum(`20`),
    `21`=sum(`21`),
    `22`=sum(`22`),
    `23`=sum(`23`)
  ),by=location_id]
  save(comp, file="comp.RData")
} else {
  load("comp.RData")
}

comp.short <- melt(comp, id.var="location_id", variable.name = "hour", value.name="count")[count != 0]
comp.short$count <- as.double(comp.short$count)
comp.short$hour <- as.integer(comp.short$hour)
setkey(comp.short, location_id, hour)
comp.short[, count := count/max(count), by=location_id][,event := "usage" ]
compbyID <- acast(comp.short, location_id ~ hour, value.var = "count", fill = 0)

kmplotter(5, compbyID, comp.short, facet = facet_grid(. ~ cluster, scale="free"), refevent="usage")
```

These analyses identify similar kinds of locations based on their aggegrated usage profile.  There is not an obviously preferable approach to the signature we used for clustering -- *i.e.*, login and logout densities *vs.* concurrent usage density -- so we choose the usage-based clusters.

TODO: there probably could be, if we did some statistics to determine best *k*, and then did some information criterion between the two models?  Additionally, compare overlap in clusters?

If we revisit our view of the presence of locations over time in the dataset, what do we see?

```{r,echo=FALSE}
usagekm <- kmeans(compbyID, 8, nstart = 5)$cluster
km.dt <- data.table(location_id=1:length(usagekm), cluster=factor(usagekm), key=c("location_id"))
ggplot(minmax.dt[km.dt]) +
  theme_bw() +
  aes(x=location_id, ymin=t_min, ymax=t_max, color=cluster) +
  geom_linerange() +
  coord_flip() + labs(y="seconds since start", x="hotspot ordered by start time") +
  scale_color_discrete()
```

There are perhaps some trends like-location creation, indicated by the solid color bands.  However, this does not clarify what sort of locations are available throughout the whole duration of the data set.  What if we look at the availability of a type cluster at anytime?

```{r, echo=FALSE}
week.melt <- setkey(melt(weekly.dt, id.var = "location_id", variable.name = "event", value.name = "week"), location_id)[km.dt]
week.melt[, inc := ifelse(event == "creation", 1, -1)]
week.melt <- week.melt[,list(inc = sum(inc)), keyby=list(cluster, week) ]
# ggplot(week.totals[cluster==1]) + aes(x = week, y = agg, fill=factor(cluster), group=cluster) + geom_area(stat="hline")
week.melt.fill <- setkey(Reduce(function(left, right) {
  ul <- left[!(week %in% right[,unique(week)]), unique(week)]
  ur <- right[!(week %in% left[,unique(week)]), unique(week)]
  ulc <- left[,unique(cluster)]
  rbind(left, right, 
    data.table(week=ul, inc=0, cluster=right[,unique(cluster)]),
    data.table(week=rep(ur, times=length(ulc)), inc=0, cluster=rep(ulc, each=length(ur)))
  )
}, Map(function(clus) week.melt[cluster == clus], week.melt[,unique(cluster)])), cluster, week)
week.totals <- week.melt.fill[,list(agg = cumsum(inc), week), keyby=cluster]
clusorder <- week.totals[,list(mx=max(agg)),keyby=cluster][,cluster,keyby=mx]$cluster
week.totals$cluster <- factor(week.totals$cluster, levels=clusorder)
ggplot(week.totals) + aes(x = week, y = agg, fill=cluster, order=cluster) + geom_area(position="fill")
```

This suggests that, at least to a reasonable approximation, the prevalance of cluster types is constant through the range we are considering for the population synthesis.

Now, how do real users use these locations?

* * *


When generating our synthetic data, we assume locations exist before and after they join the service.  So the synthetic population may make visits to these locations outside of time when the service is operating.  We trim that activity from the synthetic data according the time that locations are active in the observed data.

```{r, echo=FALSE}
## TODO switch to log(in|out) day
limits.dt <- censor.dt[,list(first=min(login), last=max(logout)), keyby=location_id]
limits.dt
save(censor.dt, limits.dt, file = "../input/censored.Rdata")
```

Our synthetic population comprises long term users of the system.  When synthezing their behavior, we should be thinking relative to other long term users of the system.  We define *long term* as having greater than six months between first and last observed use of the system by a particular user id.

For long term users, we are interested in a few first-order behaviors: how much of their time are they using the system, and how is the time spent using the system broken up into sessions?

```{r}
reference.dt <-
  setkey(censor.dt[,
    list(
      overnight_logins = sum(login_day != logout_day),
      non_overnight_logins = sum(login_day == logout_day),
      total_session_min = sum(logout-login)/60,
      user_lifetime_days = max(logout_day) - min(login_day)
    ), by=user_id
  ][
    user_lifetime_days > 365/2
  ][,
    total_logins := overnight_logins + non_overnight_logins
  ], total_logins)
```

First, we survey the relationship between total time spent on the system, period of use, and total unique logins:

```{r,echo=FALSE}
sess_scale_min <- log10(c(10, 60, 2*60, 24*60, 7*24*60, 30*24*60, 6*30*24*60, 365*24*60))
sess_labels <- c("10 min", "1 hr", "2 hr", "1 day", "1 week", "1 month", "6 months", "1 year")

use_period_scale_day <- log10(c(0.5,1:5)*365)
use_period_labels <- c(0.5,1:5)

base.plot <- ggplot(reference.dt) + theme_bw() + 
  aes(
    color=log10(user_lifetime_days),
    y=log10(total_session_min),
    x=log10(total_logins)
  ) + 
  scale_y_continuous(
    "Cumulative Usage",
    breaks=sess_scale_min,
    labels=sess_labels
  ) +
  scale_color_continuous(
    "Usage Period (Years)",
    breaks=use_period_scale_day,
    labels=use_period_labels
  ) +
  scale_x_continuous(
    "Unique Logins",
    breaks=log10(c(10,100,1000)),
    labels=c(10,100,1000)
  ) +
  geom_point() +
  stat_smooth(method=lm, color="red")
base.plot
```

Visual inspection indicates that there is a log-linear dependency of total session time, $\sum{t_i}$, on login counts, $n_l$, and the period of activity seems to explain little.  We can check this with a linear regression:

```{r, echo=FALSE}
user.model <- lm(
  log10(total_session_min) ~ log10(user_lifetime_days) + log10(total_logins),
  reference.dt
)
summary(user.model)
```

This confirms that usage period is not substantively driving total session time - unique logins are.  However, is usage period driving the total lifetime logins?  Certainly that would seem reasonable - the longer someone uses the system, the more login opportunities they have.

```{r, echo=FALSE}
logins.plot <- ggplot(reference.dt) + theme_bw() + 
  aes(
    x=log10(user_lifetime_days),
    y=log10(total_logins)
  ) + 
  scale_x_continuous(
    "Usage Period (Years)",
    breaks=use_period_scale_day,
    labels=use_period_labels
  ) +
  scale_y_continuous(
    "Unique Logins",
    breaks=log10(c(10,100,1000)),
    labels=c(10,100,1000)
  ) +
  geom_point(alpha=0.3, size=3) +
  stat_smooth(method=lm, color="red")
logins.plot
```

There seems to be a steady trend:

```{r, echo=FALSE}
logins.model <- lm(
  log10(total_logins) ~ log10(user_lifetime_days),
  reference.dt
)
summary(logins.model)
```

However, little of the variation is explained.  By visual inspection, the variation seems to remain roughly constant with the increasing trend.  Failing to have predictive terms for that variance is not a problem, as long as we are able to replicate that amount of variation in our synthetic population.

Before we try some models to reproduce that, let's consider our user behavior a bit more closely, since the system itself is growing as time proceeds.  Particularly, we want to consider whether the growth in number of available login sites influences the number of unique logins.

Consider the number of available sites:

```{r,echo=FALSE}
site.dt <- setkey(melt(
    censor.dt[,list(end=max(logout), start=min(login)), by=location_id],
    id.var="location_id",
    value.name="time",
    variable.name = "event"),
  "time")
site.dt[,increment := ifelse(event=="end",-1,1)]
site.dt[,count := cumsum(increment)]
mintime <- site.dt$time[1]
site.dt[, time := time - mintime]
locations.plot <- ggplot(site.dt) + theme_bw() + aes(x=time/(365*24*60*60), y=count) +
  geom_step() +
  scale_x_continuous("year", limits=c(0,NA))
locations.plot
```

From this view, we see a division of eras: a period of steady growth, a period of stable turnover, then another growth period terminating in the end of the data set (where locations that likely continue to exist are censored by the end of the data set).  While we could do a more rigorous interpretation, this is a practical approximation, and we can use straightforward linear regression to fit three trend lines, assuming that the middle line should have zero slope.  To curtail the terminal region in the data, we will simply identify where the trend is monotonically declining.

```{r, echo=FALSE, eval=FALSE}
grow1 <- function(break1, dt) sum(lm(count ~ time, dt[time < break1])$residuals^2)
stable <- function(break1, break2, dt) sum(dt[(break1 <= time) & (time < break2), count - mean(count)]^2)
grow2 <- function(break2, break3, dt) sum(lm(count ~ time, dt[(break2 <= time) & (time < break3)])$residuals^2)
decline <- function(break3, dt) sum(lm(count ~ time, dt[break3 <= time])$residuals^2)
partition <- function(x, maxt) {
  ## x is logit of proportions remaining in the series
  es <- exp(x)
  bs <- es/(1+es)
  break1 <- bs[1]*maxt
  break2 <- (maxt-break1)*bs[2] + break1
  break3 <- (maxt-break2)*bs[3] + break2
  grow1(break1) + stable(break1, break2) + grow2(break2, break3) + decline(break3)
}
guessp <- c(0.5, 0.5, 0.9)
res <- with(optim(log(guessp/(1-guessp)), partition, maxt=max(site.dt$time), dt=site.dt), {
  es <- exp(par)
  ps <- es/(1+es)
  maxt <- max(site.dt$time)
  break1 <- ps[1]*maxt
  break2 <- (maxt-break1)*ps[2] + break1
  break3 <- (maxt-break2)*ps[3] + break2
  c(b1=break1, b2=break2, b3=break3)  
})
locations.plot + geom_vline(x=res/(365*24*60*60), color="red")
```

This means fitting three values: the slope in the growth region, the transition point, and the end point for stable region.



If we propose some distributions of unique logins around an increasing mean number of logins, then we can determine via maximum likelihood tests (1) what shape parameters best capture the data, and (2) which distribution models the data best.  There are a few features of the distribution to capture:

 - it should be left censored.  The outcomes are bound by $\log_{10}(2)=`r log10(2)`$ as the minimum number of logins (required to appear at least long enough to measure a lifetime in the dataset greater than a 6 month duration).

First, let's consider the Poisson distribution, with the shape parameter $\lambda$ as the rate of new sessions per available location.  That is, users of the system have an activity rate per location, so more locations.

This implies that we may be able to create synthetic activity informed by the whole dataset, independently of how long a period (within the bounds of the actual empirical data) we are simulating.

TODO: some simulation approaches, show those simulated people on that plot.

The locations have different patterns of activity, reflecting their different hours of operation.

```{r, echo=FALSE}
hours.dt <- censor.dt[,
  list(login_day, login_time, logout_day, logout_time, logout_time_adj=0),
  keyby=list(location_id, user_id)
]
hours.dt[ login_day != logout_day,
   logout_time_adj := logout_time + 24*60*60
]
hours.dt <- hours.dt[,
  list(open=min(login_time), close=max(logout_time, logout_time_adj)),
  by=list(login_day, location_id)
]
p <- ggplot(hours.dt[,
  list(med_open = median(open), med_close=median(close)), by=location_id
][, open_id:=.GRP, by=list(med_open, location_id)])+aes(x=open_id, y=med_open/3600) + geom_point() + geom_hline(yintercept=1:23, color="red") + theme_bw()
# bad.list <- with(src.dt[(logout - login) > max_real_duration,],
#   list(bad_users = unique(user_id), bad_locs = unique(location_id), bad_entries = length(user_id) )
# )
# 
# duration.censored.dt <- src.dt[(logout - login) <= max_real_duration,]
# 
# eliminated.users <- setdiff(bad.list$bad_users, unique(duration.censored.dt$user_id))
# eliminated.locs <- setdiff(bad.list$bad_locs, unique(duration.censored.dt$location_id))
# warning("eliminated ", length(eliminated.users)," users with only >24 hour sessions: ", paste(eliminated.users, collapse=" "))
# 
# user.location.count.dt <- duration.censored.dt[,
# 	list(location_count = length(unique(location_id))),
# 	by="user_id"
# ]
# valid.users <- user.location.count.dt[location_count > 1,user_id]
# 
# user_and_duration.censored.dt <- duration.censored.dt[user_id %in% valid.users,]
# 
# locs.dt <- user_and_duration.censored.dt[,
# 	list(
# 		unique_users = length(unique(user_id)),
# 		total_login_time = sum(logout - login)),
# 	by="location_id"
# ] # unique users per location
# 
# min_total_login <- 60*60 # 1 hour
# 
# invalid.locs <- locs.dt[(unique_users == 1) | (total_login_time < min_total_login), location_id] # locations with only one user, or total log in time < 1 hour
# 
# loc_user_duration.censored.dt <- user_and_duration.censored.dt[!(location_id %in% invalid.locs),]
# loc_user_duration.censored.dt[,user_id := .GRP, by=user_id]
# loc_user_duration.censored.dt[,location_id := .GRP, by=location_id]
# 
# loc_view.dt <- loc_user_duration.censored.dt[,
#   list(
#     unique_users = length(unique(user_id)),
#     total_login_time = sum(logout - login),
#     mean_login = mean(logout - login),
#     sd_login = sd(logout - login)
#   ),
#   by="location_id"]
# 
# user_view.dt <- loc_user_duration.censored.dt[,
# 	list(
# 		unique_locs = length(unique(location_id)),
# 		total_login_time = sum(logout - login),
# 		mean_login = mean(logout - login),
# 		sd_login = sd(logout - login),
#     first = min(login),
#     last = max(logout)
# 	),
# 	by="user_id"
# ]
# 
# user_view.dt[, login_proportion := total_login_time / (last-first) ]
# 
# ggplot(user_view.dt) + theme_bw() +
#   aes(x=user_id, ymin=first, ymax=last, alpha=login_proportion) + geom_linerange() + coord_flip()
# 
# 
# break_categorization <- function(dt, target, output) {
# 	bks <- hist(dt[[target]], plot = F)$breaks
# 	lbls <- paste(head(bks, -1), tail(bks, -1), sep="-")
# 	dt[[output]] <- sapply(dt[[target]], function(n) factor(lbls[which.max(n <= bks)-1], levels=lbls))
# 	dt
# }
# 
# #loc_count_breaks <- hist(user_view.dt$unique_locs, plot = F)$breaks
# 
# #loc_count_cats <- paste(head(loc_count_breaks, -1), tail(loc_count_breaks, -1), sep="-")
# 
# #user_view.dt[,loc_count_cat := sapply(unique_locs, function(n) which.max(n <= loc_count_breaks)-1)]
# #user_view.dt$loc_count_cat <- factor(loc_count_cats[user_view.dt$loc_count_cat], levels=loc_count_cats)
# 
# user_view.dt <- break_categorization(user_view.dt, "unique_locs", "loc_count_cat")
# 
# # sum(loc_user_duration.censored.dt[,length(unique(location_id)),by="user_id"]$V1 == 1) == 0 # => no more user ids to censor
# 
# total_login_duration <- sum(as.numeric(loc_view.dt$total_login_time))
# loc_view.dt[,proportion := total_login_time/total_login_duration]
# 
# nets.dt <- loc_user_duration.censored.dt[, list(first=min(login),last=max(logout)), by=c("user_id","location_id")]
# 
# melt_nets.dt <- melt(nets.dt, id.vars = c("user_id", "location_id"))
# melt_nets.dt$inc <- ifelse(melt_nets.dt$variable == "first", 1, -1)
# setkey(melt_nets.dt, value, user_id, location_id)
# 
# melt_nets.dt[,net_users := cumsum(inc), by="location_id"]
# melt_nets.dt[,net_locs  := cumsum(inc), by="user_id"]
# 
# acc_loc_time <- melt_nets.dt[,
# 	list(ave_concurrent_locs = 
# 	  sum(diff(value)*head(net_locs,-1)) /
# 	  sum(diff(value)*(head(net_locs,-1)>0)),
# 	  unique_locs = sum(abs(inc))/2
# 	), # exclude time intervals w/ no presence
# 	by="user_id"]
# acc_loc_time <- break_categorization(acc_loc_time, "ave_concurrent_locs","con_locs_cat")
# acc_loc_time <- break_categorization(acc_loc_time, "unique_locs","unique_locs_cat")
# 
# ggplot(acc_loc_time[ave_concurrent_locs >= 2,]) + theme_bw() +
# 	aes(x=ave_concurrent_locs, fill=unique_locs_cat) + geom_bar() +
# 	scale_x_log10() #+ scale_y_log10()
# #sample(loc_view.dt$location_id, 5, F, loc_view.dt$proportion)
# 
# p.loc <- ggplot(loc_view.dt) + theme_bw()
# p.user <- ggplot(user_view.dt) + theme_bw()
# 
# p.user + theme(panel.margin = unit(0.5, "lines")) +
# 	aes(fill=loc_count_cat, x=total_login_time/60) +
# 	facet_grid(loc_count_cat ~ ., scales="free_y", shrink=T, drop=F) +
# 	geom_bar() + scale_x_log10(name="total login time per individual, in minutes") +
# 	ylab("number of individuals") +
# 	scale_fill_discrete(name="locations per individual", drop=F) +
# 	scale_y_continuous(breaks=function(lims) c(head(lims,1), tail(lims, 1)), expand=c(0,0))
# 
# p + aes(x=(logout - login)) + geom_bar() + geom_vline(x=150, color="red") + scale_x_log10()
# 
# #####
# p + aes(x=unique_users) + geom_bar() + scale_x_log10()
# p + aes(x=total_login_time) + geom_bar() + scale_x_log10()
# 
# src.dt[,length(unique(user_id)),by="location_id"] # unique users per location
# src.dt[,length(unique(location_id)),by="user_id"] # unique locations by user
# 
# regular_users <- src.dt[,list(regular = length(location_id) > 1), by="user_id"] # users w/ more than one login
# regular_users <- regular_users[regular == T, user_id, keyby="user_id"]$user_id
```