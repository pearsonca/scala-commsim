---
output: pdf_document
---
## Analysis and plots of montreal data.

```{r, echo=FALSE, include=FALSE}
req.packages <- c("data.table", "ggplot2", "reshape2", "grid", "scales", "animation")
need.pkgs <- req.packages[!sapply(req.packages, require, character.only = T)]
if (length(need.pkgs) != 0) install.packages(need.pkgs, dependencies = T)
```

### Required packages: `r req.packages`.

Source data:

```{r, echo=FALSE}
src.dt <- 
  setkey(
    setnames(
      fread("../input/merged.o", header = F, sep=" ", colClasses = list(integer=1.4)),
      c("user_id", "location_id", "login", "logout")
    ),
    login, logout, user_id, location_id
  )
src.dt
```

```{r, echo=FALSE}
max_hours <- 24
min_logins <- 2
min_lifetime <- 30 # days
```

Trim data:

1. remove sessions exceeding `r max_hours` hours
2. remove sessions with no time (these correspond to rows in the source data with no logout time)
3. eliminate users that have fewer than `r min_logins` logins
4. eliminate users that have a *lifetime* `(max(logout) - min(login))` less than `r min_lifetime` days
4. eliminate locations that have fewer than `r min_logins` logins
5. recursively apply steps 2-3 until no users / locations eliminated

```{r, echo=FALSE}
censor.dt <- src.dt[(logout != login) & ((logout - login) <= max_hours*60*60), ]
invalid.users <- censor.dt[,list(.N, lifetime = (max(logout) - min(login))/60/60/24), by=user_id ][N < min_logins | lifetime < min_lifetime, user_id]
while(length(invalid.users) > 0) {
  censor.dt     <- censor.dt[!user_id %in% invalid.users]
  invalid.locs  <- censor.dt[,.N, by=location_id ][N < min_logins, location_id]
  censor.dt     <- censor.dt[!location_id %in% invalid.locs]
  invalid.users <- censor.dt[,list(.N, lifetime = (max(logout) - min(login))/60/60/24), by=user_id ][N < min_logins | lifetime < min_lifetime, user_id]
}
censor.dt
```

Removed `r dim(src.dt)[1] - dim(censor.dt)[1]` rows, or `r (dim(src.dt)[1] - dim(censor.dt)[1])/dim(src.dt)[1]` reduction.

Relabel source data:

1. re-assign user ids from 1, ordered by first appearance
2. re-assign location ids from 1, ordered by first appearance
3. TODO split entries that cross days (~ 3% of data; of crossing entries, most look like late PM->early AM sessions, aka bar / club visits)
4. convert login / logout times to `log(in|out)_day` and `log(in|out)_time`
5. mark data as the empirical source: set `run_id = 0`, `sample_id = 0`, `target = FALSE`.

```{r, echo=FALSE}
source("munging.R")
zeroize(breakoutDays(censor.dt[,
  user_id := .GRP, by=user_id
][,
  location_id := .GRP, by=location_id
], login_day, login_time, logout_day, logout_time, user_id, location_id))
censor.dt
```

Now we must try to understand the remaining data.

First, what happens with our hotspot locations?  We need to understand how people appear connected to them, in order to properly model how our covert agents might use them.  To do that, we need to speculate about how these hotspots are behaving, *e.g.* are they new wifi access locations? are they substituting for existing services?

We can start by looking at all of locations, and their first and last access times.

```{r, echo=FALSE}
minmax.dt <- censor.dt[,list(t_min=min(login),t_max=max(logout)), keyby=location_id]
ggplot(minmax.dt) +
  theme_bw() +
  aes(x=location_id, ymin=t_min/60/60/24/7, ymax=t_max/60/60/24/7) +
  geom_linerange() +
  coord_flip() + labs(y="weeks since start", x="hotspot")
```

The fairly consistent slope of site start times indicates a reliable creation rate.  We can investigate this further by considering the average creation rate for the whole time period, and the rolling average.

We can determine the overall average creation rate straightforwardly, by taking the number of hotspots created over the whole duration of the data set.  We should also compare that average to an aggregation (*e.g.*, the number created in each week) to get a feel for how creation varies around that.

```{r, echo=FALSE}
weekly_ave <- minmax.dt[,
  (length(unique(location_id))-2)/(max(t_max) - min(t_min))*60*60*24*7
]
weekly.dt <- minmax.dt[,
  list(
    creation = (t_min/60/60/24)%/%7,
    shutdown = (t_max/60/60/24)%/%7
  ),
  by = location_id
]
count.dt <- weekly.dt[,
  .N, by = creation
]
ggplot(count.dt) + theme_bw() + 
  aes(x=creation, y=N) +
  labs(x="week", y="hotspots created", color="ave") +
  geom_bar(stat="identity", width=1) +
  geom_hline(y = weekly_ave, color="red")
```

We can also be a bit more statistical, fitting a least squares regression model (here, creations by day instead of by week):

```{r, echo=FALSE}
creations <- setkey(censor.dt[,
  list(creation_day=min(login_day)),
  by=location_id
][,
  .N,
  by=creation_day
][,
  list(creation_day = creation_day - min(creation_day), N)
], creation_day)
zeros <- data.table(
  creation_day = 0:max(creations$creation_day),
  N = 0,
  key = "creation_day"
)
creations <- merge(creations, zeros, all=TRUE)[,
  list(N = max(N.x, N.y, na.rm=T)),
  keyby = creation_day
]
ggplot(creations) + theme_bw() +
  aes(x=creation_day, y=N) +
  stat_smooth(method="lm") + geom_point() +
  labs(x="creation day", y="hotspots created")
```

This leaves a bit to be desired - clearly, there is a mostly constant level with possibly a very small trend, but least squares is not the best fitting approach since the underlying data is restricted to be positive.  We can use maximum likelihood to better describe the number of new hotspots created per day.  We can posit some processes, like creation of new businesses, tendency to accummulate requests and complete them all on one day, *etc.* to inspire our distribution.

TODO: zero-inflated poisson?  zero-inflated negative binomial?

Unlike creation, elimination of hotspot IDs seems to be much more variable.  While they are steadily created, their lifetime after creation has a long tailed distribution.  This is further complicated by the termination of the data, left censoring many entries.

TODO: break out entries that start near the end of the dataset and highlight their contribution to the spike in low weeks.

```{r,echo=FALSE}
dt <- minmax.dt[,
  list(duration = (t_max-t_min)/60/60/24/7),
  by=location_id
]
# h <- hist(dt$duration, 0:ceiling(max(dt$duration)), plot=F)
ggplot(dt) +
  theme_bw() +
  aes(x=duration) +
  geom_bar(binwidth=1) + labs(x="weeks of hotspot life", y="hotspots with that duration")
```

The usage profile of hostspots also varies.  We can use this to categorize "types" of locations using a *k-means* approach.  First, let's look at grouping by typical *login* and *logout* times:

```{r, echo=FALSE}
base <- censor.dt[,
  list(
    login_hour  = floor(login_time/60/60),
    logout_hour = floor(logout_time/60/60)
  ),
  by = location_id
]
both <- rbind(base[,
  list(login_count=.N),
  keyby=list(location_id, login_hour)
][,
  list(
    hour=as.integer(login_hour),
    norm=login_count/max(login_count),
    event=factor("login", levels=c("logout","login"))
  ),
  keyby=location_id
],
base[,
  list(logout_count=.N),
  keyby=list(location_id, logout_hour)
][,
  list(
    hour=as.integer(logout_hour),
    norm=logout_count/max(logout_count),
    event=factor("logout", levels=c("logout","login"))
  ),
  keyby=location_id
])
setkey(both, location_id, event, hour)
# by max vs by sum?
byid <- acast(both, location_id ~ hour + event, value.var = "norm", fill = 0)

kmplotter <- function(n, castdata, src,
  facet = facet_grid(event ~ cluster, scales = "free"),
  refevent = "login"
) {
    km <- kmeans(castdata, n, nstart = 5)
    cluster.dt <- data.table(
      location_id = 1:src[,max(location_id)],
      cluster = km$cluster, 
      key="location_id"
    )
    peakhours <- src[cluster.dt][,
      list(peak=sum(hour*norm)/sum(norm)),
      by=list(event, cluster)
    ]
    setkey(peakhours, event, peak)
    ## get the order from login
    ## relabel
    reorder <- peakhours[event == refevent, order(cluster)]
    peakhours[, cluster := reorder[cluster]]
    cluster.dt[,cluster := reorder[cluster]]
  # reorder cluster ids
    ggplot(src[cluster.dt]) +
      theme_bw() + theme(panel.grid=element_blank(), panel.grid.major.y = element_line()) +
      aes(x = factor(location_id), y = factor(hour), fill = norm) + facet +
      geom_tile() + geom_hline(data=peakhours, mapping=aes(yintercept = peak), color="red") +
      scale_x_discrete("location",  labels="") +
      scale_y_discrete(name="hour") +
#      coord_trans(ytrans = shifthrs_trans(4)) +
      labs(fill="prop. max count")
}
# print(kmplotter(3))
# print(kmplotter(4))
print(kmplotter(5, byid, both)) 
# print(kmplotter(6))
# print(kmplotter(7))
# print(kmplotter(8))
# print(kmplotter(9))
# print(kmplotter(10))
# TODO: sort out clustering - either different clustering algorithm, or principled selection of k?
```

An alternative view would be to consider something like total users present in each hour.

TODO: could weight a user contribution to an hour by how much time they are present during it, but seems reasonable to just do counts at the moment.

```{r, echo=FALSE}
comp.short <- setkey(rbindlist(
  Map(function(hour) censor.dt[
    ((login_day == logout_day) & (floor(login_time/3600) <= hour) & (hour  <= floor(logout_time/3600))) |
    ((login_day != logout_day) & ((floor(login_time/3600) <= hour) | (hour  <= floor(logout_time/3600))) ),
    list(count=.N, hour=hour, event="usage"),
    by=location_id
  ], 0:23)), location_id, hour
)
comp.short[,norm := count/max(count), by=location_id]
compbyID <- acast(comp.short, location_id ~ hour, value.var = "norm", fill = 0)
kmplotter(5, compbyID, comp.short, facet = facet_grid(. ~ cluster, scale="free"), refevent="usage")
```

These analyses identify similar kinds of locations based on their aggegrated usage profile.  There is not an obviously preferable approach to the signature we used for clustering -- *i.e.*, login and logout densities *vs.* concurrent usage density -- so we choose the usage-based clusters.

TODO: there probably could be, if we did some statistics to determine best *k*, and then did some information criterion between the two models?  Additionally, compare overlap in clusters?

If we revisit our view of the presence of locations over time in the dataset, what do we see?

```{r,echo=FALSE}
usagekm <- kmeans(compbyID, 8, nstart = 5)$cluster
km.dt <- data.table(location_id=1:length(usagekm), cluster=factor(usagekm), key=c("location_id"))
ggplot(minmax.dt[km.dt]) +
  theme_bw() +
  aes(x=location_id, ymin=t_min, ymax=t_max, color=cluster) +
  geom_linerange() +
  coord_flip() + labs(y="seconds since start", x="hotspot ordered by start time") +
  scale_color_discrete()
```

There are perhaps some trends like-location creation, indicated by the solid color bands.  However, this does not clarify what sort of locations are available throughout the whole duration of the data set.  What if we look at the availability of a type cluster at anytime?

```{r, echo=FALSE}
week.melt <- setkey(melt(weekly.dt, id.var = "location_id", variable.name = "event", value.name = "week"), location_id)[km.dt]
week.melt[, inc := ifelse(event == "creation", 1, -1)]
week.melt <- week.melt[,list(inc = sum(inc)), keyby=list(cluster, week) ]
# ggplot(week.totals[cluster==1]) + aes(x = week, y = agg, fill=factor(cluster), group=cluster) + geom_area(stat="hline")
week.melt.fill <- setkey(Reduce(function(left, right) {
  ul <- left[!(week %in% right[,unique(week)]), unique(week)]
  ur <- right[!(week %in% left[,unique(week)]), unique(week)]
  ulc <- left[,unique(cluster)]
  temp <- rbind(left, right)
  if (length(ul) != 0) temp <- rbind(temp, data.table(week=ul, inc=0, cluster=right[,unique(cluster)]))
  if (length(ur) != 0) temp <- rbind(temp, data.table(week=rep(ur, times=length(ulc)), inc=0, cluster=rep(ulc, each=length(ur))))
  temp
}, Map(function(clus) week.melt[cluster == clus], week.melt[,unique(cluster)])), cluster, week)
week.totals <- week.melt.fill[,list(agg = cumsum(inc), week), keyby=cluster]
clusorder <- week.totals[,list(mx=max(agg)),keyby=cluster][,cluster,keyby=mx]$cluster
week.totals$cluster <- factor(week.totals$cluster, levels=clusorder)
ggplot(week.totals) + aes(x = week, y = agg, fill=cluster, order=cluster) + geom_area(position="fill")
```

This suggests that, at least to a reasonable approximation, the prevalance of cluster types is constant through the range we are considering for the population synthesis.

Now, how do real users use these locations?

Have user, cluster id, time in, time out

For long term users, what is the distribution of their life time cluster usage?  do they tend to have a fixed number of items from a clusters they use?  do they replace locations in a cluster?

on last question: how would we know this from data?
 - looking at a usage time series, non-overlapping / non-interleaving use of different locations from the same cluster.  replacement == first location no longer available (other users also stop using).  displacement == first location still available, user simply stops going there

 - for each user, for each cluster:
  - does a location use stop before another starts?  relaxation: does it overlap for only a short period of time, w/ declining user-time while the other increases in user time?
  - so this looks like a combinations test: for each pair of locations?
 
Alt: try to decompose users into time series.  How to compare users w/ different lifetimes?  Do the same as locations, and give them hourly signatures.
Then sort by (user) cluster, appearance in time.  display video of users, showing usage time-bars by cluster.  preset room for max number of streams that appear in a cluster?

```{r, echo=FALSE}
setkey(censor.dt, location_id)
number_of_rows <- censor.dt[km.dt][,
  list(ul = length(unique(location_id))),
  by=list(user_id, cluster)
][,
  list(nrows = sum(max(ul))),
  by=user_id
][,
  max(nrows)
]

comp.user.short <- setkey(rbindlist(
  Map(function(hour) censor.dt[
    ((login_day == logout_day) & (floor(login_time/3600) <= hour) & (hour  <= floor(logout_time/3600))) |
    ((login_day != logout_day) & ((floor(login_time/3600) <= hour) | (hour  <= floor(logout_time/3600))) ), 
    list(count=.N, hour=hour, event="usage"),
    by=user_id
  ], 0:23)), user_id, hour
)

comp.user.short[,norm := count/max(count), by=user_id]
compuserbyID <- acast(comp.user.short, user_id ~ hour, value.var = "norm", fill = 0)

kmUserPlotter <- function(n, castdata, src,
  facet = facet_grid(. ~ cluster, scales = "free"),
  refevent = "usage"
) {
    km <- kmeans(castdata, n, nstart = 5)
    cluster.dt <- data.table(
      user_id = 1:src[,max(user_id)],
      cluster = km$cluster, 
      key="user_id"
    )
    peakhours <- src[cluster.dt][,
      list(peak=sum(hour*norm)/sum(norm)),
      by=list(event, cluster)
    ]
    setkey(peakhours, event, peak)
    ## get the order from login
    ## relabel
    reorder <- peakhours[event == refevent, order(cluster)]
    peakhours[, cluster := reorder[cluster]]
    cluster.dt[,cluster := reorder[cluster]]
  # reorder cluster ids
    ggplot(src[cluster.dt]) +
      theme_bw() + theme(panel.grid=element_blank()) +
      aes(x = factor(user_id), y = factor(hour), fill = norm) + facet +
      geom_raster() + geom_hline(data=peakhours, mapping=aes(yintercept = peak), color="red") +
      scale_x_discrete("user",  labels="") +
      scale_y_discrete(name="hour") +
      labs(fill="prop. max count")
}

## let's figure out user clusters
## then join censor.dt to user clusters
## then track those clusters as:
##  panel col, color for each cluster (all in same row, since y axis shared)
##  yaxis - cluster size / total user base at that time?
##  line for current total in cluster
##  width to represent turnover?  like ymax = count + additions, ymin = count - losses
##  x axis - week; day will be too noisy w/ turnover / count?

user.clusters <- kmParse(comp.user.short, 5, key="user_id", refevent="usage")
setkey(censor.dt, user_id)
user.melt <- melt(censor.dt[user.clusters$clustering][,
  list(leave=max(logout), join=min(login)), by=list(user_id, cluster)
], id.vars = c("user_id","cluster"), value.name = "time", variable.name = "event")

agg.dt <- user.melt[, 
  week := floor(time/60/60/24/7)
][,
  inc := ifelse(event == "leave",-1,1)
][,list(inc = sum(inc)),keyby=list(week, cluster)]

agg.fill <- setkey(Reduce(function(left, right) {
  ul <- left[!(week %in% right[,unique(week)]), unique(week)]
  ur <- right[!(week %in% left[,unique(week)]), unique(week)]
  ulc <- left[,unique(cluster)]
  temp <- rbind(left, right)
  if (length(ul) != 0) temp <- rbind(temp, data.table(week=ul, inc=0, cluster=right[,unique(cluster)]))
  if (length(ur) != 0) temp <- rbind(temp, data.table(week=rep(ur, times=length(ulc)), inc=0, cluster=rep(ulc, each=length(ur))))
  temp
}, Map(function(clus) agg.dt[cluster == clus], agg.dt[,unique(cluster)])), cluster, week)

agg.final <- agg.fill[,
  list(net = sum(inc), gain = sum(inc[inc > 0]), loss = sum(inc[inc < 0])),
  by=list(week, cluster)
][,
  list(net, gain, loss, cum=cumsum(net), week),
  keyby=cluster
]

ggplot(agg.final) + theme_bw() +
  aes(fill = factor(cluster), x = week, y = cum) + geom_area(position="fill") +
  ylab("prop. users by cluster") + labs(fill="cluster") +
  coord_cartesian(ylim=c(0,1))

# animation::saveVideo({
#   
# })
#thing <- censor.dt[km.dt][,list(ul = length(unique(location_id))),by=list(user_id, cluster)][,max(ul),by=cluster]
#test_users <- censor.dt[,.N,by=user_id][which(N==median(N)), user_id]
#setkey(censor.dt, location_id)
#typical_users <- censor.dt[km.dt][user_id %in% test_users, list(cluster=unique(cluster), count=.N, first_login=min(login), last_logout=max(logout)), keyby=list(user_id, location_id)]
#typical_users[,.N,by=list(user_id,cluster)][N>1]

```

clustering locations:

 try categories of low, med, high usage rates
 vs short, med, long login times
 
 color vs y for membership
 
 locations that disappeared between intervals - add holes?
 
 rolling aggregration windows
 
 highlight changes by size (+alpha)?

```{r, echo=F}
duration_breaks <- censor.dt[,quantile(logout-login, probs=(1:2)/3)]
daily_login_breaks <- censor.dt[,.N,by=list(location_id, login_day)][,quantile(N,probs=(1:2)/3)] ## TODO zeros?

offset <- window <- 90; slide <- 30 ## TODO: go by actual months

divy <- function(dt) dt[,
  list( # use temp vars to calc actuals
    low = low/high,
    mid = (mid-low)/high,
    high = (high-mid)/high
  ),
  keyby=location_id
]

cross_group <- function(timeslice, duration_breaks, daily_login_breaks) {
  duration_group.dt <- divy(timeslice[,
    list( # initial pass to get temp variables
      low = sum((logout - login) <= duration_breaks[1]),
      mid = sum((logout - login) <= duration_breaks[2]),
      high = .N
    ), 
    keyby=location_id
  ])
  login_group.dt <- divy(timeslice[, 
    list(count = .N), 
    by=list(login_day, location_id)
  ][,
    list( # initial pass to get temp variables
      low = sum(count <= daily_login_breaks[1]),
      mid = sum(count <= daily_login_breaks[2]),
      high = .N
    ), 
    keyby=location_id
  ])
  cross_data.dt <- merge(
    melt(
      duration_group.dt, id.vars = "location_id",
      value.name="membership", variable.name="group"
    ),
    melt(
      login_group.dt, id.vars = "location_id",
      value.name="membership", variable.name="group"
    ),
    by = "location_id",
    allow.cartesian = T,
    suffixes = c(".duration",".login")
  )
}
# censor.dt[,max(login_day)]

base_scale_x <- scale_x_discrete("location",
  limits = 1:censor.dt[,max(unique(location_id))])
base_scale_y <- scale_y_continuous("membership", limits=c(0,1))

base_plot <- ggplot() + theme_bw() +
    facet_grid(group.duration ~ group.login, labeller = function(var, val) {
      paste0(val,ifelse(var == "group.duration"," duration"," logins"))
    }) +
    aes(x=location_id, y=membership.duration*membership.login,
        color = membership.duration*membership.login - membership.duration.prev*membership.login.prev) +
    base_scale_x + base_scale_y + theme(
      axis.text.x = element_blank(),
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank()
    ) +
    scale_color_gradient("change in membership", limits = c(-1,1), low="red", high="blue", na.value = "black")

prev.dt <- cross_group(censor.dt[offset <= login_day & login_day < (offset+window)], duration_breaks, daily_login_breaks)
saveGIF(for (start in seq(offset+slide, censor.dt[,max(login_day)]-window, by=slide)) {
  timeslice <- censor.dt[start <= login_day & login_day < (start+window)]
  cross_data.dt <- cross_group(timeslice, duration_breaks, daily_login_breaks)
  plotting.dt <- merge(cross_data.dt, prev.dt, by=c("location_id", "group.duration", "group.login"), all=T, suffixes = c("",".prev"))
  print(base_plot +
    geom_point(data = plotting.dt[!is.na(membership.duration.prev)], na.rm = TRUE) +
    geom_point(data = plotting.dt[is.na(membership.duration.prev)]) +
    geom_point(
      data=plotting.dt[
        is.na(membership.duration),
        list(
          membership.duration=membership.duration.prev,
          membership.login=membership.login.prev
        ),
        by=list(group.duration, group.login, location_id)
      ], color="black", shape = 1)
  )
  prev.dt <- cross_data.dt
})
```