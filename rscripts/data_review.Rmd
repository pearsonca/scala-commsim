---
output: pdf_document
---
## Analysis and plots of montreal data.

```{r, echo=FALSE, include=FALSE}
req.packages <- c("data.table", "ggplot2", "reshape2", "grid")
sapply(req.packages, require, character.only = T)
```

### Required packages: `r req.packages`.

Source data:

```{r, echo=FALSE}
src.dt <- 
  setkey(
    setnames(
      fread("../input/merged.o", header = F, sep=" ", colClasses = list(integer=1.4)),
      c("user_id", "location_id", "login", "logout")
    ),
    login, logout, user_id, location_id
  )
src.dt
```

```{r}
max_hours <- 24
min_logins <- 2
```

Trim data:

1. remove sessions exceeding `r max_hours` hours
2. remove sessions with no time (these correspond to rows in the source data with no logout time)
3. eliminate users that have fewer than `r min_logins` logins
4. eliminate locations that have fewer than `r min_logins` logins
5. recursively apply steps 2-3 until no users / locations eliminated

```{r, echo=FALSE}
censor.dt <- src.dt[(logout != login) & ((logout - login) <= max_hours*60*60), ]
invalid.users <- censor.dt[,.N, by=user_id ][N < min_logins, user_id]
while(length(invalid.users) > 0) {
  censor.dt <- censor.dt[!user_id %in% invalid.users]
  invalid.locs <- censor.dt[,.N, by=location_id ][N < min_logins, location_id]
  censor.dt <- censor.dt[!location_id %in% invalid.locs]
  invalid.users <- censor.dt[,.N, by=user_id ][N < min_logins, user_id]
}
censor.dt
```

Removed `r dim(src.dt)[1] - dim(censor.dt)[1]` rows, or `r (dim(src.dt)[1] - dim(censor.dt)[1])/dim(src.dt)[1]` reduction.

Relabel source data:

1. re-assign user ids from 1, ordered by first appearance
2. re-assign location ids from 1, ordered by first appearance
3. TODO split entries that cross days (~ 3% of data; of crossing entries, most look like late PM->early AM sessions, aka bar / club visits)
4. convert login / logout times to `log(in|out)_day` and `log(in|out)_time`
5. mark data as the empirical source: set `run_id = 0`, `sample_id = 0`, `target = FALSE`.

```{r, echo=FALSE}
source("munging.R")
setkey(breakoutDays(censor.dt[,
  user_id := .GRP, by=user_id
][,
  location_id := .GRP, by=location_id
]), login_day, login_time, logout_day, logout_time, user_id, location_id)
censor.dt
```

When generating our synthetic data, we assume locations exist before and after they join the service.  So the synthetic population may make visits to these locations outside of time when the service is operating.  We trim that activity from the synthetic data according the time that locations are active in the observed data.

```{r, echo=FALSE}
## TODO switch to log(in|out) day
limits.dt <- censor.dt[,list(first=min(login), last=max(logout)), keyby=location_id]
limits.dt
save(censor.dt, limits.dt, file = "../input/censored.Rdata")
```

Our synthetic population comprises long term users of the system.  When synthezing their behavior, we should be thinking relative to other long term users of the system.  We define *long term* as having greater than six months between first and last observed use of the system by a particular user id.

For long term users, we are interested in a few first-order behaviors: how much of their time are they using the system, and how is the time spent using the system broken up into sessions?

```{r}
reference.dt <-
  setkey(censor.dt[,
    list(
      overnight_logins = sum(login_day != logout_day),
      non_overnight_logins = sum(login_day == logout_day),
      total_session_min = sum(logout-login)/60,
      user_lifetime_days = max(logout_day) - min(login_day)
    ), by=user_id
  ][
    user_lifetime_days > 365/2
  ][,
    total_logins := overnight_logins + non_overnight_logins
  ], total_logins)
```

First, we survey the relationship between total time spent on the system, period of use, and total unique logins:

```{r,echo=FALSE}
sess_scale_min <- log10(c(10, 60, 2*60, 24*60, 7*24*60, 30*24*60, 6*30*24*60, 365*24*60))
sess_labels <- c("10 min", "1 hr", "2 hr", "1 day", "1 week", "1 month", "6 months", "1 year")

use_period_scale_day <- log10(c(0.5,1:5)*365)
use_period_labels <- c(0.5,1:5)

base.plot <- ggplot(reference.dt) + theme_bw() + 
  aes(
    color=log10(user_lifetime_days),
    y=log10(total_session_min),
    x=log10(total_logins)
  ) + 
  scale_y_continuous(
    "Cumulative Usage",
    breaks=sess_scale_min,
    labels=sess_labels
  ) +
  scale_color_continuous(
    "Usage Period (Years)",
    breaks=use_period_scale_day,
    labels=use_period_labels
  ) +
  scale_x_continuous(
    "Unique Logins",
    breaks=log10(c(10,100,1000)),
    labels=c(10,100,1000)
  ) +
  geom_point() +
  stat_smooth(method=lm, color="red")
base.plot
```

Visual inspection indicates that there is a log-linear dependency of total session time, $\sum{t_i}$, on login counts, $n_l$, and the period of activity seems to explain little.  We can check this with a linear regression:

```{r, echo=FALSE}
user.model <- lm(
  log10(total_session_min) ~ log10(user_lifetime_days) + log10(total_logins),
  reference.dt
)
summary(user.model)
```

This confirms that usage period is not substantively driving total session time - unique logins are.  However, is usage period driving the total lifetime logins?  Certainly that would seem reasonable - the longer someone uses the system, the more login opportunities they have.

```{r, echo=FALSE}
logins.plot <- ggplot(reference.dt) + theme_bw() + 
  aes(
    x=log10(user_lifetime_days),
    y=log10(total_logins)
  ) + 
  scale_x_continuous(
    "Usage Period (Years)",
    breaks=use_period_scale_day,
    labels=use_period_labels
  ) +
  scale_y_continuous(
    "Unique Logins",
    breaks=log10(c(10,100,1000)),
    labels=c(10,100,1000)
  ) +
  geom_point(alpha=0.3, size=3) +
  stat_smooth(method=lm, color="red")
logins.plot
```

There seems to be a steady trend:

```{r, echo=FALSE}
logins.model <- lm(
  log10(total_logins) ~ log10(user_lifetime_days),
  reference.dt
)
summary(logins.model)
```

However, little of the variation is explained.  By visual inspection, the variation seems to remain roughly constant with the increasing trend.  Failing to have predictive terms for that variance is not a problem, as long as we are able to replicate that amount of variation in our synthetic population.

Before we try some models to reproduce that, let's consider our user behavior a bit more closely, since the system itself is growing as time proceeds.  Particularly, we want to consider whether the growth in number of available login sites influences the number of unique logins.

Consider the number of available sites:

```{r,echo=FALSE}
site.dt <- setkey(melt(
    censor.dt[,list(end=max(logout), start=min(login)), by=location_id],
    id.var="location_id",
    value.name="time",
    variable.name = "event"),
  "time")
site.dt[,increment := ifelse(event=="end",-1,1)]
site.dt[,count := cumsum(increment)]
mintime <- site.dt$time[1]
site.dt[, time := time - mintime]
locations.plot <- ggplot(site.dt) + theme_bw() + aes(x=time/(365*24*60*60), y=count) +
  geom_step() +
  scale_x_continuous("year", limits=c(0,NA))
locations.plot
```

From this view, we see a division of eras: a period of steady growth, a period of stable turnover, then another growth period terminating in the end of the data set (where locations that likely continue to exist are censored by the end of the data set).  While we could do a more rigorous interpretation, this is a practical approximation, and we can use straightforward linear regression to fit three trend lines, assuming that the middle line should have zero slope.  To curtail the terminal region in the data, we will simply identify where the trend is monotonically declining.

```{r, echo=FALSE}
grow1 <- function(break1, dt) sum(lm(count ~ time, dt[time < break1])$residuals^2)
stable <- function(break1, break2, dt) sum(dt[(break1 <= time) & (time < break2), count - mean(count)]^2)
grow2 <- function(break2, break3, dt) sum(lm(count ~ time, dt[(break2 <= time) & (time < break3)])$residuals^2)
decline <- function(break3, dt) sum(lm(count ~ time, dt[break3 <= time])$residuals^2)
partition <- function(x, maxt) {
  ## x is logit of proportions remaining in the series
  es <- exp(x)
  bs <- es/(1+es)
  break1 <- bs[1]*maxt
  break2 <- (maxt-break1)*bs[2] + break1
  break3 <- (maxt-break2)*bs[3] + break2
  grow1(break1) + stable(break1, break2) + grow2(break2, break3) + decline(break3)
}
guessp <- c(0.5, 0.5, 0.9)
res <- with(optim(log(guessp/(1-guessp)), partition, maxt=max(site.dt$time), dt=site.dt), {
  es <- exp(par)
  ps <- es/(1+es)
  maxt <- max(site.dt$time)
  break1 <- ps[1]*maxt
  break2 <- (maxt-break1)*ps[2] + break1
  break3 <- (maxt-break2)*ps[3] + break2
  c(b1=break1, b2=break2, b3=break3)  
})
locations.plot + geom_vline(x=res/(365*24*60*60), color="red")
```

This means fitting three values: the slope in the growth region, the transition point, and the end point for stable region.



If we propose some distributions of unique logins around an increasing mean number of logins, then we can determine via maximum likelihood tests (1) what shape parameters best capture the data, and (2) which distribution models the data best.  There are a few features of the distribution to capture:

 - it should be left censored.  The outcomes are bound by $\log_{10}(2)=`r log10(2)`$ as the minimum number of logins (required to appear at least long enough to measure a lifetime in the dataset greater than a 6 month duration).

First, let's consider the Poisson distribution, with the shape parameter $\lambda$ as the rate of new sessions per available location.  That is, users of the system have an activity rate per location, so more locations.

This implies that we may be able to create synthetic activity informed by the whole dataset, independently of how long a period (within the bounds of the actual empirical data) we are simulating.

TODO: some simulation approaches, show those simulated people on that plot.

The locations have different patterns of activity, reflecting their different hours of operation.

```{r, echo=FALSE}
hours.dt <- censor.dt[,
  list(login_day, login_time, logout_day, logout_time, logout_time_adj=0),
  keyby=list(location_id, user_id)
]
hours.dt[ login_day != logout_day,
   logout_time_adj := logout_time + 24*60*60
]
hours.dt <- hours.dt[,
  list(open=min(login_time), close=max(logout_time, logout_time_adj)),
  by=list(login_day, location_id)
]
p <- ggplot(hours.dt[,
  list(med_open = median(open), med_close=median(close)), by=location_id
][, open_id:=.GRP, by=list(med_open, location_id)])+aes(x=open_id, y=med_open/3600) + geom_point() + geom_hline(yintercept=1:23, color="red") + theme_bw()
# bad.list <- with(src.dt[(logout - login) > max_real_duration,],
#   list(bad_users = unique(user_id), bad_locs = unique(location_id), bad_entries = length(user_id) )
# )
# 
# duration.censored.dt <- src.dt[(logout - login) <= max_real_duration,]
# 
# eliminated.users <- setdiff(bad.list$bad_users, unique(duration.censored.dt$user_id))
# eliminated.locs <- setdiff(bad.list$bad_locs, unique(duration.censored.dt$location_id))
# warning("eliminated ", length(eliminated.users)," users with only >24 hour sessions: ", paste(eliminated.users, collapse=" "))
# 
# user.location.count.dt <- duration.censored.dt[,
# 	list(location_count = length(unique(location_id))),
# 	by="user_id"
# ]
# valid.users <- user.location.count.dt[location_count > 1,user_id]
# 
# user_and_duration.censored.dt <- duration.censored.dt[user_id %in% valid.users,]
# 
# locs.dt <- user_and_duration.censored.dt[,
# 	list(
# 		unique_users = length(unique(user_id)),
# 		total_login_time = sum(logout - login)),
# 	by="location_id"
# ] # unique users per location
# 
# min_total_login <- 60*60 # 1 hour
# 
# invalid.locs <- locs.dt[(unique_users == 1) | (total_login_time < min_total_login), location_id] # locations with only one user, or total log in time < 1 hour
# 
# loc_user_duration.censored.dt <- user_and_duration.censored.dt[!(location_id %in% invalid.locs),]
# loc_user_duration.censored.dt[,user_id := .GRP, by=user_id]
# loc_user_duration.censored.dt[,location_id := .GRP, by=location_id]
# 
# loc_view.dt <- loc_user_duration.censored.dt[,
#   list(
#     unique_users = length(unique(user_id)),
#     total_login_time = sum(logout - login),
#     mean_login = mean(logout - login),
#     sd_login = sd(logout - login)
#   ),
#   by="location_id"]
# 
# user_view.dt <- loc_user_duration.censored.dt[,
# 	list(
# 		unique_locs = length(unique(location_id)),
# 		total_login_time = sum(logout - login),
# 		mean_login = mean(logout - login),
# 		sd_login = sd(logout - login),
#     first = min(login),
#     last = max(logout)
# 	),
# 	by="user_id"
# ]
# 
# user_view.dt[, login_proportion := total_login_time / (last-first) ]
# 
# ggplot(user_view.dt) + theme_bw() +
#   aes(x=user_id, ymin=first, ymax=last, alpha=login_proportion) + geom_linerange() + coord_flip()
# 
# 
# break_categorization <- function(dt, target, output) {
# 	bks <- hist(dt[[target]], plot = F)$breaks
# 	lbls <- paste(head(bks, -1), tail(bks, -1), sep="-")
# 	dt[[output]] <- sapply(dt[[target]], function(n) factor(lbls[which.max(n <= bks)-1], levels=lbls))
# 	dt
# }
# 
# #loc_count_breaks <- hist(user_view.dt$unique_locs, plot = F)$breaks
# 
# #loc_count_cats <- paste(head(loc_count_breaks, -1), tail(loc_count_breaks, -1), sep="-")
# 
# #user_view.dt[,loc_count_cat := sapply(unique_locs, function(n) which.max(n <= loc_count_breaks)-1)]
# #user_view.dt$loc_count_cat <- factor(loc_count_cats[user_view.dt$loc_count_cat], levels=loc_count_cats)
# 
# user_view.dt <- break_categorization(user_view.dt, "unique_locs", "loc_count_cat")
# 
# # sum(loc_user_duration.censored.dt[,length(unique(location_id)),by="user_id"]$V1 == 1) == 0 # => no more user ids to censor
# 
# total_login_duration <- sum(as.numeric(loc_view.dt$total_login_time))
# loc_view.dt[,proportion := total_login_time/total_login_duration]
# 
# nets.dt <- loc_user_duration.censored.dt[, list(first=min(login),last=max(logout)), by=c("user_id","location_id")]
# 
# melt_nets.dt <- melt(nets.dt, id.vars = c("user_id", "location_id"))
# melt_nets.dt$inc <- ifelse(melt_nets.dt$variable == "first", 1, -1)
# setkey(melt_nets.dt, value, user_id, location_id)
# 
# melt_nets.dt[,net_users := cumsum(inc), by="location_id"]
# melt_nets.dt[,net_locs  := cumsum(inc), by="user_id"]
# 
# acc_loc_time <- melt_nets.dt[,
# 	list(ave_concurrent_locs = 
# 	  sum(diff(value)*head(net_locs,-1)) /
# 	  sum(diff(value)*(head(net_locs,-1)>0)),
# 	  unique_locs = sum(abs(inc))/2
# 	), # exclude time intervals w/ no presence
# 	by="user_id"]
# acc_loc_time <- break_categorization(acc_loc_time, "ave_concurrent_locs","con_locs_cat")
# acc_loc_time <- break_categorization(acc_loc_time, "unique_locs","unique_locs_cat")
# 
# ggplot(acc_loc_time[ave_concurrent_locs >= 2,]) + theme_bw() +
# 	aes(x=ave_concurrent_locs, fill=unique_locs_cat) + geom_bar() +
# 	scale_x_log10() #+ scale_y_log10()
# #sample(loc_view.dt$location_id, 5, F, loc_view.dt$proportion)
# 
# p.loc <- ggplot(loc_view.dt) + theme_bw()
# p.user <- ggplot(user_view.dt) + theme_bw()
# 
# p.user + theme(panel.margin = unit(0.5, "lines")) +
# 	aes(fill=loc_count_cat, x=total_login_time/60) +
# 	facet_grid(loc_count_cat ~ ., scales="free_y", shrink=T, drop=F) +
# 	geom_bar() + scale_x_log10(name="total login time per individual, in minutes") +
# 	ylab("number of individuals") +
# 	scale_fill_discrete(name="locations per individual", drop=F) +
# 	scale_y_continuous(breaks=function(lims) c(head(lims,1), tail(lims, 1)), expand=c(0,0))
# 
# p + aes(x=(logout - login)) + geom_bar() + geom_vline(x=150, color="red") + scale_x_log10()
# 
# #####
# p + aes(x=unique_users) + geom_bar() + scale_x_log10()
# p + aes(x=total_login_time) + geom_bar() + scale_x_log10()
# 
# src.dt[,length(unique(user_id)),by="location_id"] # unique users per location
# src.dt[,length(unique(location_id)),by="user_id"] # unique locations by user
# 
# regular_users <- src.dt[,list(regular = length(location_id) > 1), by="user_id"] # users w/ more than one login
# regular_users <- regular_users[regular == T, user_id, keyby="user_id"]$user_id
```