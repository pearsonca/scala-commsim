\documentclass{article}

% PLOS One TEMPLATE
\usepackage{amsmath,amssymb,graphicx,cite,setspace}
\usepackage[textwidth=1.75in]{todonotes}
\usepackage[total={6in,9.5in},top=0.5in,left=0.5in,includefoot]{geometry}
\usepackage{listings}
\usepackage{url}
\usepackage{color}

% define Scala
\lstdefinelanguage{scala}{morekeywords={class,object,trait,extends,with,new,if,while,for,def,val,var,this},
otherkeywords={->,=>},
%procnamekeys={def,.},
sensitive=true,
keywordstyle=\color{magenta},
commentstyle=\color{green},
identifierstyle=\color{blue},
%procnamestyle=\color{black},
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morestring=[b]"}
% Default settings for code listings
\lstset{frame=tb,language=scala,aboveskip=3mm,belowskip=3mm,
showstringspaces=false,columns=flexible,basicstyle={\small\ttfamily}}
% handle tilde characters
\lstset{literate=%
{~}{\url{~}}1
}
% define no break code listing
\lstnewenvironment{code}[1][]%
{
   \noindent
   \minipage{\linewidth} 
   \vspace{0.5\baselineskip}
   \lstset{basicstyle=\ttfamily\footnotesize,frame=single,#1}}
{\endminipage}

\newenvironment{rnwfig}[0]{\begin{figure}\begin{center}}{\end{center}\end{figure}}

%\renewcommand{\lstlistingname}{Scala Code}

%\doublespacing

% Text layout
%\topmargin 0.0cm
%\oddsidemargin 0.5cm
%\evensidemargin 0.5cm
%\textwidth 16cm 
%\textheight 21cm

\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

\newcommand{\Hub}[0]{\ensuremath{\mathbf{H}}}
\newcommand{\C}[1]{\ensuremath{\mathbf{C}_{#1}}}
\newcommand{\Obs}[0]{\ensuremath{\mathbf{O}}}
\newcommand{\todoCP}[1]{\todo{CP, #1}}

\usepackage{Sweave}
\begin{document}
\input{manuscript-concordance}
\begin{flushleft}
{\Large
\textbf{Detecting Covert Groups Embedded in a Population}
}
% Insert Author names, affiliations and corresponding author email.
\\
Carl A. B. Pearson$^{1,\ast}$, 
Edo Airoldi$^{2}$, 
Edward Kao$^{2}$,
Burton Singer$^{1}$, 
\\
\bf{1} Emerging Pathogens Institute, University of Florida, Gainesville, FL, USA
\\
\bf{2} Statistics, Harvard University, Cambridge, MA, USA
\\
$\ast$ E-mail: cap10@ufl.edu
\end{flushleft}
% Please keep the abstract between 250 and 300 words
\section*{Abstract}
We outline the problem of characterizing strategies for detection and concealment of clandestine coordination within a broader population, in terms of network models.  Specifically, we present (i) means to accommodate the uncertainty about behavior and capability of both covert groups and survillience efforts, (ii) a general outline for developing models to describe to describe those entities and background populations, (iii) some best practices for model parameterization and data gathering, and (iv) finally some particularly pernicious pitfalls.

As a practical, but mostly pedagogical demonstration, we specify a graph-based model of simple communication across a procedurally generated population, with an embedded, relatively small module representing a clandestine group, pitted against surveillance systems.  We discuss measuring performance of the opposing sides (e.g. Receiver Operator Characteristic), fitting the model against to real data, and finally indiciating how this model can be extended.

%\todoCP{{\em I think not for this round, but maybe:} Finally, we consider the implications of {\em forged} messages.  In the basic model, we consider incomplete information about the communications, but the available information is always accurate.  In this extension, we allow the Observer and the clandestine group to forge messages.  We again measure various Observer performance traits relative to properties of the observed network.}

\newpage

\section*{Introduction}
For investigators ranging from anthropologists to law enforcement, the need to identify groups operating in secret -- through deliberate action or otherwise -- is paramount.  In particular, the need for intelligence organizations to identify terrorist cells and defuse their violent plots is a matter of increasing import.  Symmetrically, being able to operate clandestinely in an age of ubiquitous monitoring is invaluable, for criminal organizations certainly, but also for groups subject to government abuses or businesses targeted by espionage.

In the modern era, the underlying drive for these opposed efforts (leaving aside ideology and the cases of passively hidden cohorts) is the implacable expansion of the byte trail.  Compared to past ages where the recorded information of an entire life might amount only to a few bytes -- parish records on births and deaths, perhaps including morbidity\todoCP{the right word?} -- the tools of the information era produce a near endless supply.  Cellular phones transmit constant location data, transactions with even the most remote subsidiary of a company leave trails in their logistics records, and of course any use of the internet produces veritable bit contrails.  This rate of production far exceeds the direct processing capability of any practically-sized team of analysts.

Hence, these teams employ computer-based, heuristic filtration to decide which data to record, to review, and to obtain.  We avoid saying ``algorithmic'' at this point, though these teams may themselves use that term.  ``Algorithmic'' implies a false certainty about patterns in and quality of the data associated with these analysis activities.

Given the real uncertainty, what these filters call for is testing and validation, but those present their own difficulties.  Calling field testing ``problematic'' seems like a gross understatement; reference ``truth'' is either non-existent or deceptive, and experiments could have dangerous side effects.  Even making use of intensely studied historical events is problematic: these offer no way to consider evolutionary behavior or technological innovation, even assuming the historical data are more than victor's embellishment.

Generating synthetic data seems like an obvious alternative.  It allows for comparison across both detection and masking strategies, consideration of multiple background contexts, forecasting of risks and tradeoffs in a way that allows uncertainties, and in general providing a framework for imaginative assessment.  Like all such flexible tools producing quantitative results, it has the subtle downside of the simulator's biases being validated by numerical gospel; if one believes a particular strategy is effective -- perhaps even with reasonable evidence for a particular time and situation -- the would be a natural tendency to ``adjust'' scenarios until they indicated the success of strategy.

In the following sections, we lay out the uses and abuses of such a framework.  What makes for useful synthetic data sets? What are the appropriate measures for detection strategies on them?  We motivate that discussion by inspiration from a simple, network-based model of terrorism -- a sub-group of the Salafi jihad networks as described by Sageman {\em et al.}\cite{sageman} -- and community organization and communication.  Whether or not that work is an accurate description is not of particular concern.  Their qualitative properties are enough of a defensible testbed for modeling the communication patterns of one covert group. We will point out where assumptions can be modified to identify different kinds of groups against a background population, since the behavior and structure of both the background and covert organizations are constantly evolving. 

\section*{Framing a Covert Group Model}
To test strategies for either the covert group or survillience entity, one must have a model capable of representing their strategies.  That means modeling entities that take action, modeling how that action is observed (including if it is observed correctly, or at all), and modeling how those observations are digested into reactions.  Notably, the entities must include some sort of background -- if the only data being simulated is to do with the covert entities, they are hardly covert within that simulation.  Here, we will focus on network based models of these components.  Networks seem like a natural tool, given the role of individually-based action, discrete events, and the relatively small number of participants in these groups.  Though we do not do so in our example, the background population might be more tractably modeled with continuous phenomena, given its large size and potentially more homogeneous behavior.

\subsection*{Modeling the Entities}
For our motivating example, we divide the population into three types, two of which belong to the covert group -- management and subordinates -- and a third representing ordinary individuals in the background population.  We choose this number of types because we are using that many models of activity, though different degree nodes will present somewhat differently.  The background population may be less homogenous in types, or perhaps types may be better modeled as being selected from distribution of features.  What must be guarded against here is over fitting by mechanism -- essentially the problem of choosing a polynomial of power equal to the available data, but more subtle.  Fishing with different mechanics may yield a better historical fit, but not necessarily a better forecast.

\subsubsection*{Background Population}
Most observable action will be that of the general populace surrounding the clandestine group.  This population has some structural component -- {\em e.g.} family sizes, typical numbers of working members, tendency towards assortativity -- though not necessarily well-known when a given investigation begins and perhaps even assumed to be something it is not.  This structure is also likely dynamic due to natural evolution (or activity on the structure may change pattern dynamically, those perspectives not being easy to disentangle), or possibly in response to the investigation.  For example, the ongoing revelations about the NSA will no doubt influence the behavior of the technical elite and percolate into the general public.  Therefore, assessment of any particular pair of opposed strategies should cut across multiple models of the background, each independently parametrized around what data is available.

As an example background population, we have the ordinary individuals form small groups, which in turn connect into larger groups, those groups into still larger groups, and so on until the background consists of a single component.  If one were inclined to require that this description corresponded to a particular mechanism, this might loosely be interpreted as individuals forming households, households forming blocks, blocks forming neighborhoods, {\em ad nauseum}.  However, here it is only an academic fiction -- a compact, algorithmically and analytically convenient expression, without any connection to well-established mechanics or data.  If demographic data for households were available, then we could plausibly parameterize the lowest level, then possibly combine that with mortality and mobility data to characterize how closely connected households remained, and so on.

Independently, we establish a second set of edges with a different flavor.  The previously described edges we label ``Familial'', these we call ``Economic''.  We will generate these in an identical fashion.  Again, if one were inclined to propose an explanation, one might call these small businesses or groups within a business, those forming collaborating businesses or whole firms, and so on hierarchically.  Again, we emphasize: this choice is purely an academic fiction, where we have added this extra fiction purely to highlight the need for multiple dimensions to represent different kinds of relationships in the population.

For both of these types, the ``grouping'' operation is to try to form cliques of size $n$ (with allowance to handle an arbitrary total population size).  That is:\begin{enumerate}
\item create and randomly permute a population $P_0$, with $N$ members (later results use $N=100$).
\item\label{grouping_init} divide $P_0$ into equal groups of size $n$ (later results use $n=3$);
\item for each group $i$, completely connect the individuals, and label that group $C_i^0$
\item\label{grouping_end} form a population from the $C_i^0$
\item repeat steps \ref{grouping_init} to \ref{grouping_end} with the $C_i^0$ connecting each edge between the $C_i^0$ to a uniformly drawn individual within the group, then with the $C_i^1$, etc until a single component is obtained
\end{enumerate}

\begin{code}[title=Pseudo-Code: Hierarchical Cliques]
// form a clique from a vertex collection, col:
def clique(col : Collection[V]) =
    for ( (left, right) <- undirectedPairs(col) ) // for each unique, undirected pair in col:
      left <~> right // form a bidirectional edge across the pair
      
def hierarchicalClique(col: Collection[V], size:Int) = 
   val thisLvl : Collection[Collection[V]] =
     // make the lowest level cliques 
     for ( subGroup <- groupBySize(col, size) ) yield {
       clique(subGroup)
       subGroup
     }
   cliqueGroups(thisLvl, size)
   
  val thisLvl : Collection[Collection[V]] =
    // gather the groups being made into a higher level cliques
    for ( subGroups <- groupBySize(col, size) ) yield {
      // clique a group of groups
      for ( (leftGroup, rightGroup) <- undirectedPairs(subGroup) ) {
        leftGroup.randomMember <~> rightGroup.randomMember
      }
      // merge this set of subgroups into a single new group
      merge(subGroups)
    }
  // repeat with the cliqued cliques, unless everything has been connected
  if (thisLvl.size != 1) cliqueGroups(thisLvl, size)  
\end{code}

Lastly for this model, we establish a final set of edges with a third flavor: ``Religious''.  These edges occur between members with a probability based the distance between the individuals on the ``Familial'' graph.  That is -- for those wanting to assign a meaning -- members of the same family are most likely to observably interact in a religious capacity, then immediate relatives or neighbors, and so on.  Of course, this is again academic, chosen to illustrate that other generation algorithms are possible, even with dependence between dimensions.  The detailed algorithm is:\begin{enumerate}
\item\label{religious_init} for each individual $i$:
\item assign $d_i=1$ and $F_i$ to the set of all their familial connections, excluding individuals already considered
\item with probability $p^{d_i}$ connect $i$ to the members of $F_i$
\item\label{religious_end} increment $d_i$, move all of $F_i$ to $P_i$, then add all of the familial connections of $P_i$ that are not in $P_i$ (or previously considered) to $F_i$.
\item repeat steps \ref{religious_init} to \ref{religious_end} until $F_i$ has no members added in step \ref{religious_end} 
\end{enumerate}

